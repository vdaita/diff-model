/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.396, 'grad_norm': 0.24301590025424957, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0008928571428571428}
{'loss': 1.4233, 'grad_norm': 0.24594655632972717, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0017857142857142857}
{'loss': 1.4572, 'grad_norm': 0.22077800333499908, 'learning_rate': 3e-06, 'epoch': 0.0026785714285714286}
{'loss': 1.3045, 'grad_norm': 0.4144764542579651, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0035714285714285713}
{'loss': 1.3469, 'grad_norm': 0.22991974651813507, 'learning_rate': 5e-06, 'epoch': 0.004464285714285714}
{'loss': 1.2239, 'grad_norm': 0.21052305400371552, 'learning_rate': 6e-06, 'epoch': 0.005357142857142857}
{'loss': 1.355, 'grad_norm': 0.2164614498615265, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.00625}
{'loss': 1.2998, 'grad_norm': 0.2747620642185211, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.007142857142857143}
{'loss': 1.3036, 'grad_norm': 0.20123253762722015, 'learning_rate': 9e-06, 'epoch': 0.008035714285714285}
{'loss': 1.3596, 'grad_norm': 0.23490001261234283, 'learning_rate': 1e-05, 'epoch': 0.008928571428571428}
{'loss': 1.3189, 'grad_norm': 0.23301878571510315, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.009821428571428571}
{'loss': 1.2185, 'grad_norm': 0.23445595800876617, 'learning_rate': 1.2e-05, 'epoch': 0.010714285714285714}
{'loss': 1.3111, 'grad_norm': 0.22972212731838226, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.011607142857142858}
{'loss': 1.5613, 'grad_norm': 0.2458903044462204, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0125}
{'loss': 1.3213, 'grad_norm': 0.26940855383872986, 'learning_rate': 1.5e-05, 'epoch': 0.013392857142857142}
