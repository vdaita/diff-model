[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                                                                                                                                                                                  | 0/37 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-07-01 17:26:21,046] [INFO] [axolotl.callbacks.on_train_begin:770] [PID:3853294] [RANK:0] The Axolotl config has been saved to the WandB run under files.
[2024-07-01 17:26:21,048] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:3853294] [RANK:0] packing_efficiency_estimate: 0.96 total_num_tokens per device: 1209613
[2024-07-01 17:26:21,050] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:3853294] [RANK:0] packing_efficiency_estimate: 0.96 total_num_tokens per device: 1209613
{'loss': 0.6668, 'grad_norm': 0.06000334769487381, 'learning_rate': 2e-05, 'epoch': 0.03}
  3%|█████▍                                                                                                                                                                                                    | 1/37 [01:19<47:38, 79.40s/it]






















































 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 55/56 [05:11<00:05,  5.72s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [05:17<00:00,  5.76s/it]

{'eval_loss': 0.7461267113685608, 'eval_runtime': 323.0571, 'eval_samples_per_second': 0.696, 'eval_steps_per_second': 0.176, 'epoch': 0.03}
[2024-07-01 17:34:26,969] [INFO] [axolotl.callbacks.on_step_end:125] [PID:3853294] [RANK:0] GPU memory usage while training: 7.892GB (+12.725GB cache)

  5%|██████████▊                                                                                                                                                                                            | 2/37 [08:05<2:38:33, 271.83s/it]

  8%|████████████████▏                                                                                                                                                                                      | 3/37 [09:30<1:45:34, 186.32s/it]

 11%|█████████████████████▌                                                                                                                                                                                 | 4/37 [10:55<1:20:30, 146.38s/it]

 14%|██████████████████████████▉                                                                                                                                                                            | 5/37 [12:20<1:06:11, 124.11s/it]

 16%|████████████████████████████████▌                                                                                                                                                                        | 6/37 [13:44<57:10, 110.66s/it]

 19%|██████████████████████████████████████                                                                                                                                                                   | 7/37 [15:08<50:59, 101.97s/it]


 24%|█████████████████████████████████████████████████▏                                                                                                                                                        | 9/37 [17:57<43:12, 92.59s/it]
{'loss': 0.5384, 'grad_norm': 0.12878084182739258, 'learning_rate': 0.00018, 'epoch': 0.24}

 27%|██████████████████████████████████████████████████████▎                                                                                                                                                  | 10/37 [19:21<40:28, 89.93s/it]






















































 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 55/56 [05:15<00:05,  5.81s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [05:21<00:00,  5.82s/it]

{'eval_loss': 0.45858803391456604, 'eval_runtime': 327.4151, 'eval_samples_per_second': 0.687, 'eval_steps_per_second': 0.174, 'epoch': 0.26}

 30%|██████████████████████████████████████████████████████████▊                                                                                                                                           | 11/37 [26:13<1:21:36, 188.34s/it]

 32%|████████████████████████████████████████████████████████████████▏                                                                                                                                     | 12/37 [27:37<1:05:21, 156.85s/it]

 35%|██████████████████████████████████████████████████████████████████████▎                                                                                                                                 | 13/37 [29:03<54:03, 135.15s/it]

 38%|███████████████████████████████████████████████████████████████████████████▋                                                                                                                            | 14/37 [30:28<46:00, 120.02s/it]

 41%|█████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 15/37 [31:53<40:09, 109.51s/it]

 43%|██████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                 | 16/37 [33:18<35:43, 102.05s/it]

 46%|████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                            | 17/37 [34:43<32:22, 97.10s/it]


 51%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                 | 19/37 [37:33<27:15, 90.86s/it]
{'loss': 0.2672, 'grad_norm': 0.06012401357293129, 'learning_rate': 0.00015000000000000001, 'epoch': 0.5}
 54%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                            | 20/37 [38:57<25:11, 88.89s/it]






















































 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 55/56 [05:15<00:05,  5.80s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [05:21<00:00,  5.82s/it]

{'eval_loss': 0.24859268963336945, 'eval_runtime': 327.3187, 'eval_samples_per_second': 0.687, 'eval_steps_per_second': 0.174, 'epoch': 0.53}

 57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                      | 21/37 [45:49<49:33, 185.85s/it]

 59%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                 | 22/37 [47:14<38:54, 155.60s/it]


 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                      | 24/37 [50:04<25:53, 119.47s/it]
{'loss': 0.2838, 'grad_norm': 0.03193233534693718, 'learning_rate': 9.418551710895243e-05, 'epoch': 0.64}

 68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                | 25/37 [51:28<21:47, 108.97s/it]


 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 27/37 [54:18<16:06, 96.67s/it]
{'loss': 0.2333, 'grad_norm': 0.030436988919973373, 'learning_rate': 6.039202339608432e-05, 'epoch': 0.72}

 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                 | 28/37 [55:43<13:59, 93.24s/it]

 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                           | 29/37 [57:08<12:05, 90.68s/it]

 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                      | 30/37 [58:33<10:22, 88.97s/it]






















































 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 55/56 [05:17<00:05,  5.84s/it]


100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [05:23<00:00,  5.85s/it]
{'eval_loss': 0.22415944933891296, 'eval_runtime': 329.7004, 'eval_samples_per_second': 0.682, 'eval_steps_per_second': 0.173, 'epoch': 0.79}


 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 32/37 [1:06:53<13:01, 156.30s/it]
{'loss': 0.2198, 'grad_norm': 0.026801805943250656, 'learning_rate': 1.6451218858706374e-05, 'epoch': 0.85}

 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                     | 33/37 [1:08:18<08:59, 134.84s/it]

 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 34/37 [1:09:43<05:59, 119.85s/it]

 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 35/37 [1:11:08<03:38, 109.42s/it]

 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 36/37 [1:12:33<01:42, 102.10s/it]
{'loss': 0.2324, 'grad_norm': 0.026682227849960327, 'learning_rate': 0.0, 'epoch': 0.98}
{'train_runtime': 4440.5897, 'train_samples_per_second': 0.963, 'train_steps_per_second': 0.008, 'train_loss': 0.3756845847174928, 'epoch': 0.98}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37/37 [1:13:58<00:00, 97.02s/it]/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37/37 [1:13:59<00:00, 119.98s/it]
/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
(PeftModelForCausalLM(   (base_model): LoraModel(     (model): LlamaForCausalLM(       (model): LlamaModel(         (embed_tokens): Embedding(32256, 4096)         (layers): ModuleList(           (0-31): 32 x LlamaDecoderLayer(             (self_attn): LlamaAttention(               (q_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (k_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (v_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (o_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (rotary_emb): LlamaLinearScalingRotaryEmbedding()             )             (mlp): LlamaMLP(               (gate_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (up_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (down_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=11008, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (act_fn): SiLU()             )             (input_layernorm): LlamaRMSNorm()             (post_attention_layernorm): LlamaRMSNorm()           )         )         (norm): LlamaRMSNorm()       )       (lm_head): Linear(in_features=4096, out_features=32256, bias=False)     )   ) ), LlamaTokenizerFast(name_or_path='deepseek-ai/deepseek-coder-6.7b-instruct', vocab_size=32000, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<|EOT|>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ 	32000: AddedToken("õ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32001: AddedToken("÷", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32002: AddedToken("Á", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32003: AddedToken("ý", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32004: AddedToken("À", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32005: AddedToken("ÿ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32006: AddedToken("ø", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32007: AddedToken("ú", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32008: AddedToken("þ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32009: AddedToken("ü", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32010: AddedToken("ù", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32011: AddedToken("ö", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32012: AddedToken("û", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32013: AddedToken("<｜begin▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32014: AddedToken("<｜end▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32015: AddedToken("<｜fim▁hole｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32016: AddedToken("<｜fim▁begin｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32017: AddedToken("<｜fim▁end｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32018: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32019: AddedToken("<|User|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32020: AddedToken("<|Assistant|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32021: AddedToken("<|EOT|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), })