[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                                                           | 0/60 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-07-05 12:47:39,450] [INFO] [axolotl.callbacks.on_train_begin:770] [PID:1484933] [RANK:0] The Axolotl config has been saved to the WandB run under files.
[2024-07-05 12:47:39,453] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1484933] [RANK:0] packing_efficiency_estimate: 0.95 total_num_tokens per device: 1901609
[2024-07-05 12:47:39,455] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1484933] [RANK:0] packing_efficiency_estimate: 0.95 total_num_tokens per device: 1901609
  2%|█▎                                                                               | 1/60 [01:21<1:20:06, 81.47s/it]
{'loss': 0.2054, 'grad_norm': 0.4217391610145569, 'learning_rate': 2e-05, 'epoch': 0.02}



























































































































 99%|███████████████████████████████████████████████████████████████████████████████▎| 124/125 [11:49<00:05,  5.73s/it]

100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [11:55<00:00,  5.74s/it]

{'eval_loss': 0.23535150289535522, 'eval_runtime': 721.0921, 'eval_samples_per_second': 0.693, 'eval_steps_per_second': 0.173, 'epoch': 0.02}
[2024-07-05 13:02:25,736] [INFO] [axolotl.callbacks.on_step_end:125] [PID:1484933] [RANK:0] GPU memory usage while training: 9.324GB (+12.202GB cache)

  3%|██▋                                                                             | 2/60 [14:46<8:10:04, 506.97s/it]


  7%|█████▎                                                                          | 4/60 [17:35<3:28:43, 223.63s/it]
{'loss': 0.2211, 'grad_norm': 0.6277275085449219, 'learning_rate': 8e-05, 'epoch': 0.07}


 10%|████████                                                                        | 6/60 [20:25<2:09:06, 143.45s/it]
{'loss': 0.1797, 'grad_norm': 0.717988133430481, 'learning_rate': 0.00012, 'epoch': 0.1}

 12%|█████████▎                                                                      | 7/60 [21:50<1:49:50, 124.36s/it]


 15%|████████████                                                                    | 9/60 [24:40<1:27:51, 103.36s/it]

 17%|█████████████▎                                                                  | 10/60 [26:04<1:21:07, 97.35s/it]
{'loss': 0.1266, 'grad_norm': 0.7514194846153259, 'learning_rate': 0.0002, 'epoch': 0.17}


 20%|████████████████                                                                | 12/60 [28:54<1:12:46, 90.97s/it]
{'loss': 0.0919, 'grad_norm': 0.4562687277793884, 'learning_rate': 0.0001992114701314478, 'epoch': 0.2}

 22%|█████████████████▎                                                              | 13/60 [30:18<1:09:44, 89.04s/it]

 23%|██████████████████▋                                                             | 14/60 [31:44<1:07:24, 87.92s/it]

 25%|████████████████████                                                            | 15/60 [33:10<1:05:31, 87.37s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████▎| 124/125 [11:55<00:05,  5.74s/it]

100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [12:00<00:00,  5.76s/it]

{'eval_loss': 0.06509317457675934, 'eval_runtime': 726.6957, 'eval_samples_per_second': 0.688, 'eval_steps_per_second': 0.172, 'epoch': 0.25}


 28%|██████████████████████▍                                                        | 17/60 [48:05<2:51:10, 238.84s/it]
{'loss': 0.0538, 'grad_norm': 0.25676679611206055, 'learning_rate': 0.00019048270524660196, 'epoch': 0.28}

 30%|███████████████████████▋                                                       | 18/60 [49:30<2:14:47, 192.56s/it]


 33%|██████████████████████████▎                                                    | 20/60 [52:19<1:31:38, 137.45s/it]
{'loss': 0.05, 'grad_norm': 0.11775476485490799, 'learning_rate': 0.00018090169943749476, 'epoch': 0.33}

 35%|███████████████████████████▋                                                   | 21/60 [53:44<1:19:05, 121.67s/it]

 37%|████████████████████████████▉                                                  | 22/60 [55:09<1:10:03, 110.63s/it]

 38%|██████████████████████████████▎                                                | 23/60 [56:34<1:03:25, 102.86s/it]

 40%|████████████████████████████████▊                                                 | 24/60 [57:58<58:26, 97.40s/it]

 42%|██████████████████████████████████▏                                               | 25/60 [59:23<54:33, 93.53s/it]

 43%|██████████████████████████████████▋                                             | 26/60 [1:00:47<51:24, 90.72s/it]

 45%|████████████████████████████████████                                            | 27/60 [1:02:12<48:53, 88.91s/it]

 47%|█████████████████████████████████████▎                                          | 28/60 [1:03:37<46:48, 87.75s/it]

 48%|██████████████████████████████████████▋                                         | 29/60 [1:05:02<44:53, 86.89s/it]

 50%|████████████████████████████████████████                                        | 30/60 [1:06:26<43:02, 86.07s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████▎| 124/125 [11:52<00:05,  5.71s/it]

100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [11:58<00:00,  5.73s/it]

{'eval_loss': 0.03696448355913162, 'eval_runtime': 723.9472, 'eval_samples_per_second': 0.691, 'eval_steps_per_second': 0.173, 'epoch': 0.5}

 52%|███████████████████████████████████████▊                                     | 31/60 [1:19:54<2:26:20, 302.76s/it]


 55%|██████████████████████████████████████████▎                                  | 33/60 [1:22:44<1:26:12, 191.56s/it]

 57%|███████████████████████████████████████████▋                                 | 34/60 [1:24:08<1:09:03, 159.38s/it]

 58%|██████████████████████████████████████████████                                 | 35/60 [1:25:32<56:59, 136.78s/it]
{'loss': 0.0255, 'grad_norm': 0.061825230717659, 'learning_rate': 0.0001, 'epoch': 0.58}

 60%|███████████████████████████████████████████████▍                               | 36/60 [1:26:56<48:24, 121.04s/it]

 62%|████████████████████████████████████████████████▋                              | 37/60 [1:28:21<42:12, 110.12s/it]

 63%|██████████████████████████████████████████████████                             | 38/60 [1:29:45<37:31, 102.36s/it]


 67%|█████████████████████████████████████████████████████▎                          | 40/60 [1:32:34<31:03, 93.18s/it]
{'loss': 0.0195, 'grad_norm': 0.05540106073021889, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.67}

 68%|██████████████████████████████████████████████████████▋                         | 41/60 [1:33:59<28:42, 90.64s/it]


 72%|█████████████████████████████████████████████████████████▎                      | 43/60 [1:36:47<24:42, 87.23s/it]
{'loss': 0.0184, 'grad_norm': 0.05640421807765961, 'learning_rate': 5.182463258982846e-05, 'epoch': 0.72}

 73%|██████████████████████████████████████████████████████████▋                     | 44/60 [1:38:11<23:02, 86.41s/it]

 75%|████████████████████████████████████████████████████████████                    | 45/60 [1:39:36<21:26, 85.77s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████▎| 124/125 [11:55<00:05,  5.76s/it]

100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [12:00<00:00,  5.77s/it]

{'eval_loss': 0.021803582087159157, 'eval_runtime': 726.6572, 'eval_samples_per_second': 0.688, 'eval_steps_per_second': 0.172, 'epoch': 0.75}

 77%|███████████████████████████████████████████████████████████                  | 46/60 [1:53:06<1:10:45, 303.26s/it]


 80%|███████████████████████████████████████████████████████████████▏               | 48/60 [1:55:56<38:22, 191.84s/it]
{'loss': 0.0177, 'grad_norm': 0.049334216862916946, 'learning_rate': 2.7103137257858868e-05, 'epoch': 0.8}


 83%|█████████████████████████████████████████████████████████████████▊             | 50/60 [1:58:46<22:53, 137.37s/it]

 85%|███████████████████████████████████████████████████████████████████▏           | 51/60 [2:00:10<18:13, 121.50s/it]
{'loss': 0.0178, 'grad_norm': 0.04265137016773224, 'learning_rate': 1.5567207449798515e-05, 'epoch': 0.85}

 87%|████████████████████████████████████████████████████████████████████▍          | 52/60 [2:01:35<14:43, 110.49s/it]

 88%|█████████████████████████████████████████████████████████████████████▊         | 53/60 [2:02:59<11:58, 102.64s/it]

 90%|████████████████████████████████████████████████████████████████████████        | 54/60 [2:04:24<09:42, 97.16s/it]

 92%|█████████████████████████████████████████████████████████████████████████▎      | 55/60 [2:05:49<07:48, 93.64s/it]

 93%|██████████████████████████████████████████████████████████████████████████▋     | 56/60 [2:07:13<06:02, 90.69s/it]

 95%|████████████████████████████████████████████████████████████████████████████    | 57/60 [2:08:37<04:26, 88.84s/it]

 97%|█████████████████████████████████████████████████████████████████████████████▎  | 58/60 [2:10:02<02:54, 87.40s/it]


100%|████████████████████████████████████████████████████████████████████████████████| 60/60 [2:12:50<00:00, 85.94s/it]
{'loss': 0.0174, 'grad_norm': 0.04234748333692551, 'learning_rate': 0.0, 'epoch': 1.0}




























































































































100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [11:53<00:00,  5.71s/it]

  warnings.warn(
{'eval_loss': 0.020185451954603195, 'eval_runtime': 718.9946, 'eval_samples_per_second': 0.695, 'eval_steps_per_second': 0.174, 'epoch': 1.0}
{'train_runtime': 8694.6576, 'train_samples_per_second': 0.518, 'train_steps_per_second': 0.007, 'train_loss': 0.058110755992432435, 'epoch': 1.0}
100%|███████████████████████████████████████████████████████████████████████████████| 60/60 [2:24:53<00:00, 144.89s/it]
[2024-07-05 15:12:54,118] [INFO] [axolotl.train.log:61] [PID:1484933] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/dscoder-code-ellipsis
/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(









adapter_model.bin:  99%|███████████████████████████████████████████████████████████▎| 838M/848M [00:18<00:00, 44.3MB/s]

adapter_model.bin: 100%|████████████████████████████████████████████████████████████| 848M/848M [00:19<00:00, 44.6MB/s]