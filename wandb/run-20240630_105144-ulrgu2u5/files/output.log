[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                                                                          | 0/156 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-06-30 10:51:45,237] [INFO] [axolotl.callbacks.on_train_begin:770] [PID:1945781] [RANK:0] The Axolotl config has been saved to the WandB run under files.
[2024-06-30 10:51:45,240] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1945781] [RANK:0] packing_efficiency_estimate: 0.93 total_num_tokens per device: 1209613
[2024-06-30 10:51:45,243] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1945781] [RANK:0] packing_efficiency_estimate: 0.93 total_num_tokens per device: 1209613
  1%|▌                                                                                               | 1/156 [00:38<1:39:39, 38.58s/it]
  0%|                                                                                                           | 0/56 [00:00<?, ?it/s]






















































 98%|████████████████████████████████████████████████████████████████████████████████████████████████▎ | 55/56 [02:26<00:02,  2.74s/it]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [02:28<00:00,  2.76s/it]

{'eval_loss': 0.7461566925048828, 'eval_runtime': 151.7973, 'eval_samples_per_second': 1.482, 'eval_steps_per_second': 0.376, 'epoch': 0.01}
[2024-06-30 10:55:36,000] [INFO] [axolotl.callbacks.on_step_end:125] [PID:1945781] [RANK:0] GPU memory usage while training: 7.638GB (+10.245GB cache)

  1%|█▏                                                                                             | 2/156 [03:50<5:30:55, 128.93s/it]

  2%|█▊                                                                                              | 3/156 [04:31<3:46:03, 88.65s/it]

  3%|██▍                                                                                             | 4/156 [05:12<2:56:59, 69.87s/it]

  3%|███                                                                                             | 5/156 [05:53<2:29:20, 59.34s/it]

  4%|███▋                                                                                            | 6/156 [06:34<2:12:44, 53.09s/it]

  4%|████▎                                                                                           | 7/156 [07:14<2:01:42, 49.01s/it]

  5%|████▉                                                                                           | 8/156 [07:55<1:54:09, 46.28s/it]

  6%|█████▌                                                                                          | 9/156 [08:35<1:49:06, 44.54s/it]

  6%|██████                                                                                         | 10/156 [09:16<1:45:11, 43.23s/it]

  7%|██████▋                                                                                        | 11/156 [09:56<1:42:37, 42.46s/it]

  8%|███████▎                                                                                       | 12/156 [10:37<1:40:40, 41.95s/it]

  8%|███████▉                                                                                       | 13/156 [11:18<1:39:10, 41.61s/it]

  9%|████████▌                                                                                      | 14/156 [11:58<1:37:29, 41.20s/it]

 10%|█████████▏                                                                                     | 15/156 [12:39<1:36:24, 41.02s/it]

 10%|█████████▋                                                                                     | 16/156 [13:20<1:35:36, 40.98s/it]

 11%|██████████▎                                                                                    | 17/156 [14:00<1:34:33, 40.82s/it]


 12%|███████████▌                                                                                   | 19/156 [15:21<1:32:50, 40.66s/it]
{'loss': 0.3306, 'grad_norm': 0.09768980741500854, 'learning_rate': 0.00019, 'epoch': 0.24}

 13%|████████████▏                                                                                  | 20/156 [16:02<1:32:06, 40.63s/it]

 13%|████████████▊                                                                                  | 21/156 [16:42<1:31:26, 40.64s/it]

 14%|█████████████▍                                                                                 | 22/156 [17:23<1:30:42, 40.62s/it]

 15%|██████████████                                                                                 | 23/156 [18:03<1:29:55, 40.57s/it]

 15%|██████████████▌                                                                                | 24/156 [18:44<1:28:58, 40.45s/it]

 16%|███████████████▏                                                                               | 25/156 [19:25<1:28:41, 40.62s/it]


 17%|████████████████▍                                                                              | 27/156 [20:46<1:27:08, 40.53s/it]
{'loss': 0.2806, 'grad_norm': 0.0666637197136879, 'learning_rate': 0.00019869550768882455, 'epoch': 0.35}

 18%|█████████████████                                                                              | 28/156 [21:26<1:26:29, 40.54s/it]

 19%|█████████████████▋                                                                             | 29/156 [22:07<1:25:58, 40.62s/it]


 20%|██████████████████▉                                                                            | 31/156 [23:28<1:24:22, 40.50s/it]
{'loss': 0.2049, 'grad_norm': 0.0376153439283371, 'learning_rate': 0.00019678900739873226, 'epoch': 0.4}

 21%|███████████████████▍                                                                           | 32/156 [24:08<1:23:38, 40.47s/it]

 21%|████████████████████                                                                           | 33/156 [24:48<1:22:54, 40.44s/it]

 22%|████████████████████▋                                                                          | 34/156 [25:28<1:21:57, 40.31s/it]

 22%|█████████████████████▎                                                                         | 35/156 [26:09<1:21:28, 40.40s/it]

 23%|█████████████████████▉                                                                         | 36/156 [26:50<1:20:54, 40.46s/it]

 24%|██████████████████████▌                                                                        | 37/156 [27:30<1:20:12, 40.45s/it]

 24%|███████████████████████▏                                                                       | 38/156 [28:11<1:19:39, 40.50s/it]

 25%|███████████████████████▊                                                                       | 39/156 [28:51<1:19:04, 40.55s/it]






















































 98%|████████████████████████████████████████████████████████████████████████████████████████████████▎ | 55/56 [02:28<00:02,  2.75s/it]
[2024-06-30 11:23:09,672] [INFO] [accelerate.accelerator.log:61] [PID:1945781] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.

  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2528, 'grad_norm': 0.03256627917289734, 'learning_rate': 0.00018951632913550626, 'epoch': 0.51}
 26%|████████████████████████▎                                                                      | 40/156 [32:09<2:49:14, 87.54s/it]

 26%|████████████████████████▉                                                                      | 41/156 [32:49<2:20:47, 73.45s/it]

 27%|█████████████████████████▌                                                                     | 42/156 [33:30<2:00:42, 63.53s/it]

 28%|██████████████████████████▏                                                                    | 43/156 [34:10<1:46:41, 56.65s/it]

 28%|██████████████████████████▊                                                                    | 44/156 [34:51<1:37:02, 51.99s/it]

 29%|███████████████████████████▍                                                                   | 45/156 [35:32<1:30:00, 48.65s/it]

 29%|████████████████████████████                                                                   | 46/156 [36:13<1:24:52, 46.29s/it]

 30%|████████████████████████████▌                                                                  | 47/156 [36:54<1:21:07, 44.66s/it]

 31%|█████████████████████████████▏                                                                 | 48/156 [37:34<1:18:06, 43.39s/it]

 31%|█████████████████████████████▊                                                                 | 49/156 [38:15<1:15:57, 42.59s/it]

 32%|██████████████████████████████▍                                                                | 50/156 [38:56<1:14:13, 42.02s/it]

 33%|███████████████████████████████                                                                | 51/156 [39:36<1:12:44, 41.57s/it]

 33%|███████████████████████████████▋                                                               | 52/156 [40:16<1:11:13, 41.09s/it]

 34%|████████████████████████████████▎                                                              | 53/156 [40:57<1:10:17, 40.94s/it]

 35%|████████████████████████████████▉                                                              | 54/156 [41:37<1:09:17, 40.76s/it]

 35%|█████████████████████████████████▍                                                             | 55/156 [42:18<1:08:32, 40.72s/it]

 36%|██████████████████████████████████                                                             | 56/156 [42:58<1:07:52, 40.73s/it]

 37%|██████████████████████████████████▋                                                            | 57/156 [43:39<1:07:15, 40.77s/it]

 37%|███████████████████████████████████▎                                                           | 58/156 [44:20<1:06:43, 40.85s/it]

 38%|███████████████████████████████████▉                                                           | 59/156 [45:01<1:05:50, 40.72s/it]

 38%|████████████████████████████████████▌                                                          | 60/156 [45:41<1:05:03, 40.66s/it]

 39%|█████████████████████████████████████▏                                                         | 61/156 [46:22<1:04:22, 40.66s/it]

 40%|█████████████████████████████████████▊                                                         | 62/156 [47:02<1:03:32, 40.56s/it]

 40%|██████████████████████████████████████▎                                                        | 63/156 [47:43<1:02:59, 40.64s/it]

 41%|██████████████████████████████████████▉                                                        | 64/156 [48:23<1:02:04, 40.48s/it]

 42%|███████████████████████████████████████▌                                                       | 65/156 [49:03<1:01:15, 40.39s/it]

 42%|████████████████████████████████████████▏                                                      | 66/156 [49:44<1:00:45, 40.50s/it]

 43%|█████████████████████████████████████████▋                                                       | 67/156 [50:24<59:56, 40.41s/it]

 44%|██████████████████████████████████████████▎                                                      | 68/156 [51:04<59:10, 40.35s/it]

 44%|██████████████████████████████████████████▉                                                      | 69/156 [51:45<58:25, 40.30s/it]

 45%|███████████████████████████████████████████▌                                                     | 70/156 [52:25<57:54, 40.40s/it]

 46%|████████████████████████████████████████████▏                                                    | 71/156 [53:06<57:09, 40.34s/it]

 46%|████████████████████████████████████████████▊                                                    | 72/156 [53:46<56:23, 40.28s/it]

 47%|█████████████████████████████████████████████▍                                                   | 73/156 [54:26<55:48, 40.34s/it]

 47%|██████████████████████████████████████████████                                                   | 74/156 [55:07<55:14, 40.43s/it]

 48%|██████████████████████████████████████████████▋                                                  | 75/156 [55:48<55:01, 40.76s/it]


 49%|███████████████████████████████████████████████▉                                                 | 77/156 [57:09<53:33, 40.67s/it]
{'loss': 0.2207, 'grad_norm': 0.033331647515296936, 'learning_rate': 0.0001251373834511103, 'epoch': 0.98}
 50%|████████████████████████████████████████████████▌                                                | 78/156 [57:50<52:57, 40.74s/it]






















































 98%|████████████████████████████████████████████████████████████████████████████████████████████████▎ | 55/56 [02:28<00:02,  2.74s/it]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [02:30<00:00,  2.76s/it]
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.1755, 'grad_norm': 0.03746604546904564, 'learning_rate': 0.00012064051010900397, 'epoch': 1.01}
 51%|███████████████████████████████████████████████                                              | 79/156 [1:01:06<1:51:58, 87.25s/it]

 51%|███████████████████████████████████████████████▋                                             | 80/156 [1:01:47<1:32:47, 73.25s/it]
[2024-06-30 11:53:35,655] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1945781] [RANK:0] packing_efficiency_estimate: 0.93 total_num_tokens per device: 1209613
[2024-06-30 11:53:35,658] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1945781] [RANK:0] packing_efficiency_estimate: 0.93 total_num_tokens per device: 1209613

 52%|████████████████████████████████████████████████▎                                            | 81/156 [1:02:28<1:19:39, 63.73s/it]


 53%|█████████████████████████████████████████████████▍                                           | 83/156 [1:03:50<1:03:19, 52.05s/it]
{'loss': 0.1592, 'grad_norm': 0.03295598179101944, 'learning_rate': 0.00011152431031326978, 'epoch': 1.04}

 54%|███████████████████████████████████████████████████▏                                           | 84/156 [1:04:31<58:26, 48.70s/it]

 54%|███████████████████████████████████████████████████▊                                           | 85/156 [1:05:12<54:55, 46.42s/it]

 55%|████████████████████████████████████████████████████▎                                          | 86/156 [1:05:52<52:02, 44.61s/it]

 56%|████████████████████████████████████████████████████▉                                          | 87/156 [1:06:33<49:54, 43.40s/it]

 56%|█████████████████████████████████████████████████████▌                                         | 88/156 [1:07:13<48:06, 42.44s/it]

 57%|██████████████████████████████████████████████████████▏                                        | 89/156 [1:07:54<46:48, 41.92s/it]

 58%|██████████████████████████████████████████████████████▊                                        | 90/156 [1:08:34<45:34, 41.44s/it]

 58%|███████████████████████████████████████████████████████▍                                       | 91/156 [1:09:15<44:38, 41.21s/it]

 59%|████████████████████████████████████████████████████████                                       | 92/156 [1:09:55<43:49, 41.08s/it]

 60%|████████████████████████████████████████████████████████▋                                      | 93/156 [1:10:36<42:57, 40.90s/it]

 60%|█████████████████████████████████████████████████████████▏                                     | 94/156 [1:11:17<42:24, 41.04s/it]

 61%|█████████████████████████████████████████████████████████▊                                     | 95/156 [1:11:58<41:30, 40.82s/it]


 62%|███████████████████████████████████████████████████████████                                    | 97/156 [1:13:18<39:55, 40.61s/it]
{'loss': 0.1812, 'grad_norm': 0.03325138986110687, 'learning_rate': 7.935948989099605e-05, 'epoch': 1.21}

 63%|███████████████████████████████████████████████████████████▋                                   | 98/156 [1:13:59<39:09, 40.51s/it]

 63%|████████████████████████████████████████████████████████████▎                                  | 99/156 [1:14:39<38:28, 40.50s/it]

 64%|████████████████████████████████████████████████████████████▎                                 | 100/156 [1:15:20<37:49, 40.53s/it]

 65%|████████████████████████████████████████████████████████████▊                                 | 101/156 [1:16:00<37:10, 40.55s/it]

 65%|█████████████████████████████████████████████████████████████▍                                | 102/156 [1:16:41<36:32, 40.60s/it]

 66%|██████████████████████████████████████████████████████████████                                | 103/156 [1:17:22<35:53, 40.62s/it]

 67%|██████████████████████████████████████████████████████████████▋                               | 104/156 [1:18:03<35:15, 40.69s/it]

 67%|███████████████████████████████████████████████████████████████▎                              | 105/156 [1:18:44<34:38, 40.75s/it]

 68%|███████████████████████████████████████████████████████████████▊                              | 106/156 [1:19:25<34:01, 40.83s/it]

 69%|████████████████████████████████████████████████████████████████▍                             | 107/156 [1:20:06<33:24, 40.91s/it]

 69%|█████████████████████████████████████████████████████████████████                             | 108/156 [1:20:47<32:42, 40.89s/it]

 70%|█████████████████████████████████████████████████████████████████▋                            | 109/156 [1:21:28<32:07, 41.00s/it]

 71%|██████████████████████████████████████████████████████████████████▎                           | 110/156 [1:22:09<31:23, 40.95s/it]


 72%|███████████████████████████████████████████████████████████████████▍                          | 112/156 [1:23:31<30:09, 41.12s/it]
{'loss': 0.1686, 'grad_norm': 0.03349510580301285, 'learning_rate': 4.735678371226441e-05, 'epoch': 1.41}


 73%|████████████████████████████████████████████████████████████████████▋                         | 114/156 [1:24:53<28:44, 41.06s/it]
{'loss': 0.1462, 'grad_norm': 0.035017482936382294, 'learning_rate': 4.3486358557740814e-05, 'epoch': 1.43}


 74%|█████████████████████████████████████████████████████████████████████▉                        | 116/156 [1:26:15<27:21, 41.04s/it]
{'loss': 0.1798, 'grad_norm': 0.037266362458467484, 'learning_rate': 3.973653636207437e-05, 'epoch': 1.46}
 75%|██████████████████████████████████████████████████████████████████████▌                       | 117/156 [1:26:56<26:39, 41.02s/it]























































100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [02:32<00:00,  2.76s/it]

[2024-06-30 12:21:15,285] [INFO] [accelerate.accelerator.log:61] [PID:1945781] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.
 75%|██████████████████████████████████████████████████████████████████████▌                       | 117/156 [1:29:32<26:39, 41.02s/it]/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2053, 'grad_norm': 0.03466426581144333, 'learning_rate': 3.6115319434803894e-05, 'epoch': 1.48}
 76%|███████████████████████████████████████████████████████████████████████                       | 118/156 [1:30:14<55:43, 87.98s/it]

 76%|███████████████████████████████████████████████████████████████████████▋                      | 119/156 [1:30:55<45:36, 73.95s/it]

 77%|████████████████████████████████████████████████████████████████████████▎                     | 120/156 [1:31:36<38:25, 64.03s/it]

 78%|████████████████████████████████████████████████████████████████████████▉                     | 121/156 [1:32:17<33:18, 57.11s/it]

 78%|█████████████████████████████████████████████████████████████████████████▌                    | 122/156 [1:32:58<29:35, 52.21s/it]

 79%|██████████████████████████████████████████████████████████████████████████                    | 123/156 [1:33:39<26:52, 48.87s/it]

 79%|██████████████████████████████████████████████████████████████████████████▋                   | 124/156 [1:34:20<24:45, 46.42s/it]

 80%|███████████████████████████████████████████████████████████████████████████▎                  | 125/156 [1:35:00<23:05, 44.70s/it]

 81%|███████████████████████████████████████████████████████████████████████████▉                  | 126/156 [1:35:41<21:48, 43.63s/it]

 81%|████████████████████████████████████████████████████████████████████████████▌                 | 127/156 [1:36:23<20:43, 42.89s/it]

 82%|█████████████████████████████████████████████████████████████████████████████▏                | 128/156 [1:37:03<19:41, 42.20s/it]

 83%|█████████████████████████████████████████████████████████████████████████████▋                | 129/156 [1:37:44<18:50, 41.86s/it]

 83%|██████████████████████████████████████████████████████████████████████████████▎               | 130/156 [1:38:25<18:03, 41.69s/it]

 84%|██████████████████████████████████████████████████████████████████████████████▉               | 131/156 [1:39:06<17:14, 41.36s/it]

 85%|███████████████████████████████████████████████████████████████████████████████▌              | 132/156 [1:39:47<16:28, 41.17s/it]

 85%|████████████████████████████████████████████████████████████████████████████████▏             | 133/156 [1:40:28<15:44, 41.05s/it]

 86%|████████████████████████████████████████████████████████████████████████████████▋             | 134/156 [1:41:08<15:01, 40.99s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████▎            | 135/156 [1:41:49<14:20, 40.96s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████▉            | 136/156 [1:42:30<13:38, 40.91s/it]

 88%|██████████████████████████████████████████████████████████████████████████████████▌           | 137/156 [1:43:11<12:57, 40.94s/it]

 88%|███████████████████████████████████████████████████████████████████████████████████▏          | 138/156 [1:43:52<12:15, 40.85s/it]

 89%|███████████████████████████████████████████████████████████████████████████████████▊          | 139/156 [1:44:32<11:32, 40.74s/it]

 90%|████████████████████████████████████████████████████████████████████████████████████▎         | 140/156 [1:45:13<10:52, 40.75s/it]

 90%|████████████████████████████████████████████████████████████████████████████████████▉         | 141/156 [1:45:54<10:11, 40.78s/it]

 91%|█████████████████████████████████████████████████████████████████████████████████████▌        | 142/156 [1:46:35<09:30, 40.78s/it]

 92%|██████████████████████████████████████████████████████████████████████████████████████▏       | 143/156 [1:47:15<08:48, 40.67s/it]

 92%|██████████████████████████████████████████████████████████████████████████████████████▊       | 144/156 [1:47:56<08:08, 40.75s/it]

 93%|███████████████████████████████████████████████████████████████████████████████████████▎      | 145/156 [1:48:37<07:29, 40.88s/it]

 94%|███████████████████████████████████████████████████████████████████████████████████████▉      | 146/156 [1:49:18<06:48, 40.88s/it]

 94%|████████████████████████████████████████████████████████████████████████████████████████▌     | 147/156 [1:49:59<06:08, 40.98s/it]

 95%|█████████████████████████████████████████████████████████████████████████████████████████▏    | 148/156 [1:50:40<05:26, 40.87s/it]

 96%|█████████████████████████████████████████████████████████████████████████████████████████▊    | 149/156 [1:51:21<04:46, 40.87s/it]

 96%|██████████████████████████████████████████████████████████████████████████████████████████▍   | 150/156 [1:52:02<04:05, 40.96s/it]

 97%|██████████████████████████████████████████████████████████████████████████████████████████▉   | 151/156 [1:52:43<03:25, 41.01s/it]

 97%|███████████████████████████████████████████████████████████████████████████████████████████▌  | 152/156 [1:53:24<02:44, 41.11s/it]

 98%|████████████████████████████████████████████████████████████████████████████████████████████▏ | 153/156 [1:54:06<02:03, 41.11s/it]

 99%|████████████████████████████████████████████████████████████████████████████████████████████▊ | 154/156 [1:54:47<01:22, 41.16s/it]

 99%|█████████████████████████████████████████████████████████████████████████████████████████████▍| 155/156 [1:55:28<00:41, 41.31s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [1:56:10<00:00, 41.37s/it]






















































 98%|████████████████████████████████████████████████████████████████████████████████████████████████▎ | 55/56 [02:30<00:02,  2.76s/it]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [02:33<00:00,  2.78s/it]
  warnings.warn(
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [1:58:48<00:00, 45.69s/it]
/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'train_runtime': 7129.2438, 'train_samples_per_second': 1.199, 'train_steps_per_second': 0.022, 'train_loss': 0.2558478829570306, 'epoch': 1.97}
[2024-06-30 12:50:33,295] [INFO] [axolotl.train.log:61] [PID:1945781] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/axolotl-qlora-out-code-ir-dscoder
(PeftModelForCausalLM(   (base_model): LoraModel(     (model): LlamaForCausalLM(       (model): LlamaModel(         (embed_tokens): Embedding(32256, 4096)         (layers): ModuleList(           (0-31): 32 x LlamaDecoderLayer(             (self_attn): LlamaAttention(               (q_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (k_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (v_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (o_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (rotary_emb): LlamaLinearScalingRotaryEmbedding()             )             (mlp): LlamaMLP(               (gate_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (up_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (down_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=11008, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (act_fn): SiLU()             )             (input_layernorm): LlamaRMSNorm()             (post_attention_layernorm): LlamaRMSNorm()           )         )         (norm): LlamaRMSNorm()       )       (lm_head): Linear(in_features=4096, out_features=32256, bias=False)     )   ) ), LlamaTokenizerFast(name_or_path='deepseek-ai/deepseek-coder-6.7b-instruct', vocab_size=32000, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<|EOT|>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ 	32000: AddedToken("õ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32001: AddedToken("÷", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32002: AddedToken("Á", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32003: AddedToken("ý", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32004: AddedToken("À", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32005: AddedToken("ÿ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32006: AddedToken("ø", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32007: AddedToken("ú", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32008: AddedToken("þ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32009: AddedToken("ü", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32010: AddedToken("ù", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32011: AddedToken("ö", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32012: AddedToken("û", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32013: AddedToken("<｜begin▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32014: AddedToken("<｜end▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32015: AddedToken("<｜fim▁hole｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32016: AddedToken("<｜fim▁begin｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32017: AddedToken("<｜fim▁end｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32018: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32019: AddedToken("<|User|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32020: AddedToken("<|Assistant|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32021: AddedToken("<|EOT|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), })