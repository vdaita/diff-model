/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4051, 'grad_norm': 0.18987908959388733, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0017857142857142857}
{'loss': 1.3557, 'grad_norm': 0.18754354119300842, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0035714285714285713}
{'loss': 1.2643, 'grad_norm': 0.18721634149551392, 'learning_rate': 3e-06, 'epoch': 0.005357142857142857}
{'loss': 1.3142, 'grad_norm': 0.1907849758863449, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.007142857142857143}
{'loss': 1.3706, 'grad_norm': 0.19217221438884735, 'learning_rate': 5e-06, 'epoch': 0.008928571428571428}
{'loss': 1.298, 'grad_norm': 0.2765962779521942, 'learning_rate': 6e-06, 'epoch': 0.010714285714285714}
{'loss': 1.4519, 'grad_norm': 0.19651344418525696, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0125}
{'loss': 1.325, 'grad_norm': 0.22100123763084412, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.014285714285714285}
{'loss': 1.3377, 'grad_norm': 0.19863863289356232, 'learning_rate': 9e-06, 'epoch': 0.01607142857142857}
{'loss': 1.279, 'grad_norm': 0.19224391877651215, 'learning_rate': 1e-05, 'epoch': 0.017857142857142856}
{'loss': 1.3006, 'grad_norm': 0.2039991021156311, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.019642857142857142}
{'loss': 1.3818, 'grad_norm': 0.21348261833190918, 'learning_rate': 1.2e-05, 'epoch': 0.02142857142857143}
{'loss': 1.3463, 'grad_norm': 0.22541974484920502, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.023214285714285715}
{'loss': 1.3266, 'grad_norm': 1.0364632606506348, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.025}
{'loss': 1.3643, 'grad_norm': 0.23536795377731323, 'learning_rate': 1.5e-05, 'epoch': 0.026785714285714284}
{'loss': 1.4387, 'grad_norm': 0.2595910131931305, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.02857142857142857}
{'loss': 1.3193, 'grad_norm': 0.231744185090065, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.030357142857142857}
{'loss': 1.2398, 'grad_norm': 0.2168445885181427, 'learning_rate': 1.8e-05, 'epoch': 0.03214285714285714}
{'loss': 1.4419, 'grad_norm': 0.2745130956172943, 'learning_rate': 1.9e-05, 'epoch': 0.033928571428571426}
{'loss': 1.3065, 'grad_norm': 0.6664897799491882, 'learning_rate': 2e-05, 'epoch': 0.03571428571428571}
{'loss': 1.3928, 'grad_norm': 0.2837516665458679, 'learning_rate': 2.1e-05, 'epoch': 0.0375}
{'loss': 1.3967, 'grad_norm': 0.5521336793899536, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.039285714285714285}
{'loss': 1.2223, 'grad_norm': 0.2314676195383072, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.04107142857142857}
{'loss': 1.277, 'grad_norm': 0.2721893787384033, 'learning_rate': 2.4e-05, 'epoch': 0.04285714285714286}
{'loss': 1.2527, 'grad_norm': 0.2538343071937561, 'learning_rate': 2.5e-05, 'epoch': 0.044642857142857144}
{'loss': 1.274, 'grad_norm': 0.25355175137519836, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.04642857142857143}
{'loss': 1.3371, 'grad_norm': 0.28181010484695435, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.048214285714285716}
{'loss': 1.2058, 'grad_norm': 0.2816307544708252, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.05}
{'loss': 1.2661, 'grad_norm': 0.3091014623641968, 'learning_rate': 2.9e-05, 'epoch': 0.05178571428571429}
{'loss': 1.2988, 'grad_norm': 0.35132765769958496, 'learning_rate': 3e-05, 'epoch': 0.05357142857142857}
{'loss': 1.1758, 'grad_norm': 0.289398193359375, 'learning_rate': 3.1e-05, 'epoch': 0.055357142857142855}
{'loss': 1.1107, 'grad_norm': 0.2802499532699585, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.05714285714285714}
{'loss': 1.22, 'grad_norm': 0.32529544830322266, 'learning_rate': 3.3e-05, 'epoch': 0.05892857142857143}
{'loss': 1.2126, 'grad_norm': 0.33951112627983093, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.060714285714285714}
{'loss': 1.1793, 'grad_norm': 0.29743850231170654, 'learning_rate': 3.5e-05, 'epoch': 0.0625}
{'loss': 1.285, 'grad_norm': 0.3827742636203766, 'learning_rate': 3.6e-05, 'epoch': 0.06428571428571428}
{'loss': 1.2791, 'grad_norm': 0.31601712107658386, 'learning_rate': 3.7e-05, 'epoch': 0.06607142857142857}
{'loss': 1.1577, 'grad_norm': 0.31480875611305237, 'learning_rate': 3.8e-05, 'epoch': 0.06785714285714285}
{'loss': 1.2123, 'grad_norm': 0.31886306405067444, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.06964285714285715}
{'loss': 1.1823, 'grad_norm': 0.29909926652908325, 'learning_rate': 4e-05, 'epoch': 0.07142857142857142}
{'loss': 1.0988, 'grad_norm': 0.3093195855617523, 'learning_rate': 4.1e-05, 'epoch': 0.07321428571428572}
{'loss': 1.171, 'grad_norm': 0.3075332045555115, 'learning_rate': 4.2e-05, 'epoch': 0.075}
{'loss': 1.2387, 'grad_norm': 0.3250215947628021, 'learning_rate': 4.3e-05, 'epoch': 0.07678571428571429}
{'loss': 1.1512, 'grad_norm': 0.3165840804576874, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.07857142857142857}
{'loss': 1.1266, 'grad_norm': 0.3659267723560333, 'learning_rate': 4.5e-05, 'epoch': 0.08035714285714286}
{'loss': 1.2046, 'grad_norm': 0.5725799798965454, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08214285714285714}
{'loss': 1.1654, 'grad_norm': 0.3616808354854584, 'learning_rate': 4.7e-05, 'epoch': 0.08392857142857142}
{'loss': 1.084, 'grad_norm': 0.3407531678676605, 'learning_rate': 4.8e-05, 'epoch': 0.08571428571428572}
{'loss': 1.1436, 'grad_norm': 0.29758551716804504, 'learning_rate': 4.9e-05, 'epoch': 0.0875}
{'loss': 1.1865, 'grad_norm': 0.3772814869880676, 'learning_rate': 5e-05, 'epoch': 0.08928571428571429}
{'loss': 1.1301, 'grad_norm': 0.3183954060077667, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.09107142857142857}
{'loss': 1.0982, 'grad_norm': 0.32383018732070923, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.09285714285714286}
{'loss': 1.067, 'grad_norm': 0.3886570632457733, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.09464285714285714}
{'loss': 1.0517, 'grad_norm': 0.47348520159721375, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.09642857142857143}
{'loss': 0.9574, 'grad_norm': 0.3218838572502136, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.09821428571428571}
{'loss': 1.0453, 'grad_norm': 0.3540332317352295, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.1}
