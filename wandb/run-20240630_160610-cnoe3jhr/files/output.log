[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                                                                          | 0/156 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-06-30 16:06:10,688] [INFO] [axolotl.callbacks.on_train_begin:770] [PID:2264943] [RANK:0] The Axolotl config has been saved to the WandB run under files.
[2024-06-30 16:06:10,691] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:2264943] [RANK:0] packing_efficiency_estimate: 0.93 total_num_tokens per device: 1219016
[2024-06-30 16:06:10,693] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:2264943] [RANK:0] packing_efficiency_estimate: 0.93 total_num_tokens per device: 1219016
  1%|▌                                                                                               | 1/156 [00:38<1:38:11, 38.01s/it]
{'loss': 0.838, 'grad_norm': 0.09472231566905975, 'learning_rate': 1e-05, 'epoch': 0.01}






















































 98%|████████████████████████████████████████████████████████████████████████████████████████████████▎ | 55/56 [02:24<00:02,  2.70s/it]

[2024-06-30 16:09:17,179] [INFO] [accelerate.accelerator.log:61] [PID:2264943] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.
{'eval_loss': 0.9881212115287781, 'eval_runtime': 150.4439, 'eval_samples_per_second': 1.496, 'eval_steps_per_second': 0.379, 'epoch': 0.01}
[2024-06-30 16:09:58,998] [INFO] [axolotl.callbacks.on_step_end:125] [PID:2264943] [RANK:0] GPU memory usage while training: 7.628GB (+9.097GB cache)

  1%|█▏                                                                                             | 2/156 [03:48<5:27:29, 127.59s/it]

  2%|█▊                                                                                              | 3/156 [04:28<3:43:32, 87.67s/it]

  3%|██▍                                                                                             | 4/156 [05:08<2:54:46, 68.99s/it]

  3%|███                                                                                             | 5/156 [05:49<2:28:04, 58.84s/it]

  4%|███▋                                                                                            | 6/156 [06:30<2:11:22, 52.55s/it]

  4%|████▎                                                                                           | 7/156 [07:10<2:00:17, 48.44s/it]

  5%|████▉                                                                                           | 8/156 [07:50<1:53:00, 45.81s/it]

  6%|█████▌                                                                                          | 9/156 [08:30<1:47:48, 44.01s/it]

  6%|██████                                                                                         | 10/156 [09:10<1:43:56, 42.71s/it]

  7%|██████▋                                                                                        | 11/156 [09:49<1:41:09, 41.86s/it]

  8%|███████▎                                                                                       | 12/156 [10:30<1:39:26, 41.43s/it]

  8%|███████▉                                                                                       | 13/156 [11:10<1:37:33, 40.94s/it]

  9%|████████▌                                                                                      | 14/156 [11:50<1:36:19, 40.70s/it]

 10%|█████████▏                                                                                     | 15/156 [12:30<1:35:08, 40.48s/it]

 10%|█████████▋                                                                                     | 16/156 [13:10<1:34:17, 40.41s/it]

 11%|██████████▎                                                                                    | 17/156 [13:50<1:33:09, 40.21s/it]

 12%|██████████▉                                                                                    | 18/156 [14:30<1:32:29, 40.21s/it]

 12%|███████████▌                                                                                   | 19/156 [15:10<1:31:46, 40.19s/it]

 13%|████████████▏                                                                                  | 20/156 [15:50<1:30:48, 40.06s/it]

 13%|████████████▊                                                                                  | 21/156 [16:30<1:30:08, 40.06s/it]

 14%|█████████████▍                                                                                 | 22/156 [17:10<1:29:31, 40.09s/it]

 15%|██████████████                                                                                 | 23/156 [17:50<1:28:41, 40.01s/it]

 15%|██████████████▌                                                                                | 24/156 [18:30<1:27:45, 39.89s/it]


 17%|███████████████▊                                                                               | 26/156 [19:50<1:26:28, 39.91s/it]
{'loss': 0.3217, 'grad_norm': 0.060895323753356934, 'learning_rate': 0.0001990410430875205, 'epoch': 0.33}


 18%|█████████████████                                                                              | 28/156 [21:10<1:25:18, 39.99s/it]
{'loss': 0.3229, 'grad_norm': 0.06490703672170639, 'learning_rate': 0.0001982973099683902, 'epoch': 0.36}

 19%|█████████████████▋                                                                             | 29/156 [21:49<1:24:17, 39.82s/it]

 19%|██████████████████▎                                                                            | 30/156 [22:29<1:23:30, 39.77s/it]

 20%|██████████████████▉                                                                            | 31/156 [23:09<1:22:58, 39.83s/it]

 21%|███████████████████▍                                                                           | 32/156 [23:49<1:22:30, 39.92s/it]

 21%|████████████████████                                                                           | 33/156 [24:29<1:21:48, 39.91s/it]

 22%|████████████████████▋                                                                          | 34/156 [25:09<1:21:23, 40.03s/it]

 22%|█████████████████████▎                                                                         | 35/156 [25:49<1:20:43, 40.03s/it]

 23%|█████████████████████▉                                                                         | 36/156 [26:29<1:20:15, 40.13s/it]

 24%|██████████████████████▌                                                                        | 37/156 [27:10<1:19:50, 40.25s/it]

 24%|███████████████████████▏                                                                       | 38/156 [27:50<1:19:12, 40.27s/it]
 25%|███████████████████████▊                                                                       | 39/156 [28:31<1:18:32, 40.28s/it]






















































 98%|████████████████████████████████████████████████████████████████████████████████████████████████▎ | 55/56 [02:27<00:02,  2.71s/it]
[2024-06-30 16:37:13,107] [INFO] [accelerate.accelerator.log:61] [PID:2264943] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.

  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2795, 'grad_norm': 0.03677397966384888, 'learning_rate': 0.00018951632913550626, 'epoch': 0.51}
 26%|████████████████████████▎                                                                      | 40/156 [31:46<2:47:49, 86.81s/it]

 26%|████████████████████████▉                                                                      | 41/156 [32:27<2:19:47, 72.93s/it]

 27%|█████████████████████████▌                                                                     | 42/156 [33:07<1:59:54, 63.11s/it]

 28%|██████████████████████████▏                                                                    | 43/156 [33:47<1:45:49, 56.19s/it]

 28%|██████████████████████████▊                                                                    | 44/156 [34:26<1:35:40, 51.25s/it]

 29%|███████████████████████████▍                                                                   | 45/156 [35:07<1:28:46, 47.98s/it]

 29%|████████████████████████████                                                                   | 46/156 [35:48<1:23:57, 45.79s/it]

 30%|████████████████████████████▌                                                                  | 47/156 [36:27<1:20:00, 44.04s/it]

 31%|█████████████████████████████▏                                                                 | 48/156 [37:07<1:17:03, 42.81s/it]

 31%|█████████████████████████████▊                                                                 | 49/156 [37:48<1:15:07, 42.13s/it]

 32%|██████████████████████████████▍                                                                | 50/156 [38:28<1:13:29, 41.60s/it]

 33%|███████████████████████████████                                                                | 51/156 [39:09<1:12:13, 41.27s/it]

 33%|███████████████████████████████▋                                                               | 52/156 [39:49<1:11:06, 41.02s/it]

 34%|████████████████████████████████▎                                                              | 53/156 [40:30<1:10:09, 40.87s/it]

 35%|████████████████████████████████▉                                                              | 54/156 [41:10<1:09:01, 40.61s/it]

 35%|█████████████████████████████████▍                                                             | 55/156 [41:50<1:08:05, 40.45s/it]

 36%|██████████████████████████████████                                                             | 56/156 [42:30<1:07:20, 40.41s/it]

 37%|██████████████████████████████████▋                                                            | 57/156 [43:11<1:06:46, 40.47s/it]

 37%|███████████████████████████████████▎                                                           | 58/156 [43:51<1:05:57, 40.38s/it]

 38%|███████████████████████████████████▉                                                           | 59/156 [44:31<1:05:20, 40.41s/it]

 38%|████████████████████████████████████▌                                                          | 60/156 [45:12<1:04:45, 40.48s/it]

 39%|█████████████████████████████████████▏                                                         | 61/156 [45:52<1:04:01, 40.44s/it]

 40%|█████████████████████████████████████▊                                                         | 62/156 [46:33<1:03:33, 40.57s/it]

 40%|██████████████████████████████████████▎                                                        | 63/156 [47:14<1:02:46, 40.50s/it]

 41%|██████████████████████████████████████▉                                                        | 64/156 [47:54<1:01:59, 40.43s/it]

 42%|███████████████████████████████████████▌                                                       | 65/156 [48:34<1:01:19, 40.43s/it]

 42%|████████████████████████████████████████▏                                                      | 66/156 [49:15<1:00:45, 40.51s/it]

 43%|████████████████████████████████████████▊                                                      | 67/156 [49:55<1:00:01, 40.47s/it]

 44%|██████████████████████████████████████████▎                                                      | 68/156 [50:36<59:19, 40.45s/it]

 44%|██████████████████████████████████████████▉                                                      | 69/156 [51:16<58:32, 40.37s/it]

 45%|███████████████████████████████████████████▌                                                     | 70/156 [51:56<57:52, 40.38s/it]

 46%|████████████████████████████████████████████▏                                                    | 71/156 [52:37<57:17, 40.44s/it]

 46%|████████████████████████████████████████████▊                                                    | 72/156 [53:17<56:36, 40.43s/it]

 47%|█████████████████████████████████████████████▍                                                   | 73/156 [53:58<55:55, 40.42s/it]

 47%|██████████████████████████████████████████████                                                   | 74/156 [54:39<55:28, 40.59s/it]

 48%|██████████████████████████████████████████████▋                                                  | 75/156 [55:19<54:39, 40.49s/it]

 49%|███████████████████████████████████████████████▎                                                 | 76/156 [56:00<54:01, 40.52s/it]

 49%|███████████████████████████████████████████████▉                                                 | 77/156 [56:40<53:26, 40.59s/it]
 50%|████████████████████████████████████████████████▌                                                | 78/156 [57:21<52:37, 40.48s/it]






















































 98%|████████████████████████████████████████████████████████████████████████████████████████████████▎ | 55/56 [02:28<00:02,  2.73s/it]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [02:30<00:00,  2.74s/it]
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2265, 'grad_norm': 0.03569444641470909, 'learning_rate': 0.00012064051010900397, 'epoch': 1.0}
 51%|███████████████████████████████████████████████                                              | 79/156 [1:00:36<1:51:46, 87.10s/it]

 51%|███████████████████████████████████████████████▋                                             | 80/156 [1:01:17<1:32:29, 73.02s/it]
[2024-06-30 17:07:37,728] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:2264943] [RANK:0] packing_efficiency_estimate: 0.93 total_num_tokens per device: 1219016
[2024-06-30 17:07:37,731] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:2264943] [RANK:0] packing_efficiency_estimate: 0.93 total_num_tokens per device: 1219016

 52%|████████████████████████████████████████████████▎                                            | 81/156 [1:01:57<1:18:56, 63.16s/it]

 53%|████████████████████████████████████████████████▉                                            | 82/156 [1:02:38<1:09:37, 56.46s/it]

 53%|█████████████████████████████████████████████████▍                                           | 83/156 [1:03:18<1:02:43, 51.56s/it]

 54%|███████████████████████████████████████████████████▏                                           | 84/156 [1:03:58<57:57, 48.30s/it]

 54%|███████████████████████████████████████████████████▊                                           | 85/156 [1:04:39<54:33, 46.11s/it]

 55%|████████████████████████████████████████████████████▎                                          | 86/156 [1:05:20<51:53, 44.48s/it]

 56%|████████████████████████████████████████████████████▉                                          | 87/156 [1:06:00<49:41, 43.20s/it]

 56%|█████████████████████████████████████████████████████▌                                         | 88/156 [1:06:41<47:56, 42.31s/it]

 57%|██████████████████████████████████████████████████████▏                                        | 89/156 [1:07:21<46:41, 41.82s/it]

 58%|██████████████████████████████████████████████████████▊                                        | 90/156 [1:08:01<45:26, 41.32s/it]

 58%|███████████████████████████████████████████████████████▍                                       | 91/156 [1:08:42<44:27, 41.04s/it]

 59%|████████████████████████████████████████████████████████                                       | 92/156 [1:09:22<43:31, 40.81s/it]

 60%|████████████████████████████████████████████████████████▋                                      | 93/156 [1:10:03<42:45, 40.72s/it]

 60%|█████████████████████████████████████████████████████████▏                                     | 94/156 [1:10:43<42:05, 40.74s/it]

 61%|█████████████████████████████████████████████████████████▊                                     | 95/156 [1:11:24<41:21, 40.68s/it]

 62%|██████████████████████████████████████████████████████████▍                                    | 96/156 [1:12:04<40:32, 40.55s/it]

 62%|███████████████████████████████████████████████████████████                                    | 97/156 [1:12:45<39:51, 40.53s/it]

 63%|███████████████████████████████████████████████████████████▋                                   | 98/156 [1:13:25<39:05, 40.45s/it]

 63%|████████████████████████████████████████████████████████████▎                                  | 99/156 [1:14:05<38:16, 40.29s/it]

 64%|████████████████████████████████████████████████████████████▎                                 | 100/156 [1:14:45<37:37, 40.31s/it]

 65%|████████████████████████████████████████████████████████████▊                                 | 101/156 [1:15:26<37:00, 40.38s/it]

 65%|█████████████████████████████████████████████████████████████▍                                | 102/156 [1:16:06<36:12, 40.24s/it]

 66%|██████████████████████████████████████████████████████████████                                | 103/156 [1:16:46<35:32, 40.24s/it]

 67%|██████████████████████████████████████████████████████████████▋                               | 104/156 [1:17:26<34:53, 40.26s/it]

 67%|███████████████████████████████████████████████████████████████▎                              | 105/156 [1:18:06<34:14, 40.28s/it]

 68%|███████████████████████████████████████████████████████████████▊                              | 106/156 [1:18:47<33:32, 40.24s/it]

 69%|████████████████████████████████████████████████████████████████▍                             | 107/156 [1:19:27<32:55, 40.32s/it]

 69%|█████████████████████████████████████████████████████████████████                             | 108/156 [1:20:07<32:12, 40.26s/it]

 70%|█████████████████████████████████████████████████████████████████▋                            | 109/156 [1:20:47<31:28, 40.19s/it]

 71%|██████████████████████████████████████████████████████████████████▎                           | 110/156 [1:21:27<30:42, 40.06s/it]

 71%|██████████████████████████████████████████████████████████████████▉                           | 111/156 [1:22:07<30:00, 40.01s/it]

 72%|███████████████████████████████████████████████████████████████████▍                          | 112/156 [1:22:47<29:21, 40.03s/it]

 72%|████████████████████████████████████████████████████████████████████                          | 113/156 [1:23:27<28:44, 40.10s/it]

 73%|████████████████████████████████████████████████████████████████████▋                         | 114/156 [1:24:08<28:09, 40.22s/it]

 74%|█████████████████████████████████████████████████████████████████████▎                        | 115/156 [1:24:48<27:29, 40.23s/it]

 74%|█████████████████████████████████████████████████████████████████████▉                        | 116/156 [1:25:28<26:41, 40.05s/it]

 75%|██████████████████████████████████████████████████████████████████████▌                       | 117/156 [1:26:07<25:58, 39.97s/it]






















































 98%|████████████████████████████████████████████████████████████████████████████████████████████████▎ | 55/56 [02:27<00:02,  2.72s/it]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [02:30<00:00,  2.74s/it]

  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3335, 'grad_norm': 0.0391448475420475, 'learning_rate': 3.6115319434803894e-05, 'epoch': 1.47}
 76%|███████████████████████████████████████████████████████████████████████                       | 118/156 [1:29:23<54:48, 86.53s/it]

 76%|███████████████████████████████████████████████████████████████████████▋                      | 119/156 [1:30:03<44:45, 72.57s/it]

 77%|████████████████████████████████████████████████████████████████████████▎                     | 120/156 [1:30:43<37:44, 62.91s/it]

 78%|████████████████████████████████████████████████████████████████████████▉                     | 121/156 [1:31:23<32:39, 56.00s/it]

 78%|█████████████████████████████████████████████████████████████████████████▌                    | 122/156 [1:32:03<29:00, 51.20s/it]

 79%|██████████████████████████████████████████████████████████████████████████                    | 123/156 [1:32:43<26:19, 47.86s/it]

 79%|██████████████████████████████████████████████████████████████████████████▋                   | 124/156 [1:33:23<24:15, 45.48s/it]

 80%|███████████████████████████████████████████████████████████████████████████▎                  | 125/156 [1:34:03<22:41, 43.91s/it]

 81%|███████████████████████████████████████████████████████████████████████████▉                  | 126/156 [1:34:43<21:20, 42.68s/it]

 81%|████████████████████████████████████████████████████████████████████████████▌                 | 127/156 [1:35:23<20:13, 41.84s/it]

 82%|█████████████████████████████████████████████████████████████████████████████▏                | 128/156 [1:36:03<19:17, 41.35s/it]

 83%|█████████████████████████████████████████████████████████████████████████████▋                | 129/156 [1:36:43<18:23, 40.86s/it]

 83%|██████████████████████████████████████████████████████████████████████████████▎               | 130/156 [1:37:23<17:35, 40.60s/it]

 84%|██████████████████████████████████████████████████████████████████████████████▉               | 131/156 [1:38:03<16:51, 40.46s/it]

 85%|███████████████████████████████████████████████████████████████████████████████▌              | 132/156 [1:38:42<16:04, 40.20s/it]

 85%|████████████████████████████████████████████████████████████████████████████████▏             | 133/156 [1:39:23<15:24, 40.21s/it]

 86%|████████████████████████████████████████████████████████████████████████████████▋             | 134/156 [1:40:02<14:42, 40.09s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████▎            | 135/156 [1:40:42<13:59, 39.98s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████▉            | 136/156 [1:41:23<13:22, 40.13s/it]

 88%|██████████████████████████████████████████████████████████████████████████████████▌           | 137/156 [1:42:03<12:40, 40.04s/it]

 88%|███████████████████████████████████████████████████████████████████████████████████▏          | 138/156 [1:42:43<12:03, 40.18s/it]

 89%|███████████████████████████████████████████████████████████████████████████████████▊          | 139/156 [1:43:23<11:23, 40.22s/it]

 90%|████████████████████████████████████████████████████████████████████████████████████▎         | 140/156 [1:44:03<10:42, 40.13s/it]

 90%|████████████████████████████████████████████████████████████████████████████████████▉         | 141/156 [1:44:44<10:03, 40.21s/it]

 91%|█████████████████████████████████████████████████████████████████████████████████████▌        | 142/156 [1:45:24<09:22, 40.19s/it]

 92%|██████████████████████████████████████████████████████████████████████████████████████▏       | 143/156 [1:46:04<08:42, 40.17s/it]

 92%|██████████████████████████████████████████████████████████████████████████████████████▊       | 144/156 [1:46:44<08:00, 40.06s/it]

 93%|███████████████████████████████████████████████████████████████████████████████████████▎      | 145/156 [1:47:24<07:19, 40.00s/it]

 94%|███████████████████████████████████████████████████████████████████████████████████████▉      | 146/156 [1:48:03<06:39, 39.96s/it]

 94%|████████████████████████████████████████████████████████████████████████████████████████▌     | 147/156 [1:48:43<05:59, 39.95s/it]

 95%|█████████████████████████████████████████████████████████████████████████████████████████▏    | 148/156 [1:49:24<05:20, 40.08s/it]

 96%|█████████████████████████████████████████████████████████████████████████████████████████▊    | 149/156 [1:50:04<04:41, 40.18s/it]

 96%|██████████████████████████████████████████████████████████████████████████████████████████▍   | 150/156 [1:50:44<04:00, 40.05s/it]

 97%|██████████████████████████████████████████████████████████████████████████████████████████▉   | 151/156 [1:51:24<03:20, 40.06s/it]

 97%|███████████████████████████████████████████████████████████████████████████████████████████▌  | 152/156 [1:52:04<02:40, 40.11s/it]

 98%|████████████████████████████████████████████████████████████████████████████████████████████▏ | 153/156 [1:52:44<02:00, 40.10s/it]

 99%|████████████████████████████████████████████████████████████████████████████████████████████▊ | 154/156 [1:53:25<01:20, 40.42s/it]

 99%|█████████████████████████████████████████████████████████████████████████████████████████████▍| 155/156 [1:54:06<00:40, 40.58s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [1:54:46<00:00, 40.33s/it]






















































 98%|████████████████████████████████████████████████████████████████████████████████████████████████▎ | 55/56 [02:27<00:02,  2.70s/it]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [02:30<00:00,  2.71s/it]

  warnings.warn(
{'train_runtime': 7042.4596, 'train_samples_per_second': 1.214, 'train_steps_per_second': 0.022, 'train_loss': 0.31558134043828034, 'epoch': 1.96}
[2024-06-30 18:03:31,900] [INFO] [axolotl.train.log:61] [PID:2264943] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/axolotl-qlora-out-line-dscoder
(PeftModelForCausalLM(   (base_model): LoraModel(     (model): LlamaForCausalLM(       (model): LlamaModel(         (embed_tokens): Embedding(32256, 4096)         (layers): ModuleList(           (0-31): 32 x LlamaDecoderLayer(             (self_attn): LlamaAttention(               (q_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (k_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (v_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (o_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (rotary_emb): LlamaLinearScalingRotaryEmbedding()             )             (mlp): LlamaMLP(               (gate_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (up_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (down_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=11008, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (act_fn): SiLU()             )             (input_layernorm): LlamaRMSNorm()             (post_attention_layernorm): LlamaRMSNorm()           )         )         (norm): LlamaRMSNorm()       )       (lm_head): Linear(in_features=4096, out_features=32256, bias=False)     )   ) ), LlamaTokenizerFast(name_or_path='deepseek-ai/deepseek-coder-6.7b-instruct', vocab_size=32000, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<|EOT|>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ 	32000: AddedToken("õ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32001: AddedToken("÷", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32002: AddedToken("Á", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32003: AddedToken("ý", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32004: AddedToken("À", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32005: AddedToken("ÿ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32006: AddedToken("ø", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32007: AddedToken("ú", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32008: AddedToken("þ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32009: AddedToken("ü", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32010: AddedToken("ù", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32011: AddedToken("ö", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32012: AddedToken("û", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32013: AddedToken("<｜begin▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32014: AddedToken("<｜end▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32015: AddedToken("<｜fim▁hole｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32016: AddedToken("<｜fim▁begin｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32017: AddedToken("<｜fim▁end｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32018: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32019: AddedToken("<|User|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32020: AddedToken("<|Assistant|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32021: AddedToken("<|EOT|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), })
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [1:57:21<00:00, 45.14s/it]
/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(