[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                                                                                                                                                                                  | 0/51 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-07-02 13:24:43,648] [INFO] [axolotl.callbacks.on_train_begin:770] [PID:937022] [RANK:0] The Axolotl config has been saved to the WandB run under files.
[2024-07-02 13:24:43,650] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:937022] [RANK:0] packing_efficiency_estimate: 0.96 total_num_tokens per device: 1632422
[2024-07-02 13:24:43,652] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:937022] [RANK:0] packing_efficiency_estimate: 0.96 total_num_tokens per device: 1632422
  2%|███▉                                                                                                                                                                                                    | 1/51 [01:20<1:06:47, 80.14s/it]
{'loss': 0.5233, 'grad_norm': 0.43432602286338806, 'learning_rate': 2e-05, 'epoch': 0.02}

























































































































 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 124/125 [11:47<00:05,  5.70s/it]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [11:53<00:00,  5.72s/it]

{'eval_loss': 0.5554375052452087, 'eval_runtime': 718.9, 'eval_samples_per_second': 0.696, 'eval_steps_per_second': 0.174, 'epoch': 0.02}
[2024-07-02 13:39:25,987] [INFO] [axolotl.callbacks.on_step_end:125] [PID:937022] [RANK:0] GPU memory usage while training: 9.332GB (+11.502GB cache)

  4%|███████▊                                                                                                                                                                                               | 2/51 [14:42<6:52:19, 504.88s/it]

  6%|███████████▋                                                                                                                                                                                           | 3/51 [16:07<4:10:27, 313.06s/it]

  8%|███████████████▌                                                                                                                                                                                       | 4/51 [17:32<2:54:49, 223.17s/it]

 10%|███████████████████▎                                                                                                                                                                                 | 5/51 [18:56<2:12:40, 173.06s/it]

 12%|███████████████████████▏                                                                                                                                                                             | 6/51 [20:21<1:47:09, 142.87s/it]

 14%|███████████████████████████                                                                                                                                                                          | 7/51 [21:45<1:30:44, 123.74s/it]

 16%|██████████████████████████████▉                                                                                                                                                                      | 8/51 [23:09<1:19:40, 111.17s/it]

 18%|██████████████████████████████████▊                                                                                                                                                                  | 9/51 [24:33<1:11:54, 102.72s/it]

 20%|██████████████████████████████████████▋                                                                                                                                                              | 10/51 [25:57<1:06:17, 97.01s/it]

 22%|██████████████████████████████████████████▍                                                                                                                                                          | 11/51 [27:22<1:02:02, 93.07s/it]

 24%|██████████████████████████████████████████████▊                                                                                                                                                        | 12/51 [28:45<58:34, 90.13s/it]
 25%|██████████████████████████████████████████████████▋                                                                                                                                                    | 13/51 [30:08<55:46, 88.08s/it]




























































































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [11:56<00:00,  5.76s/it]

[2024-07-02 14:06:50,747] [INFO] [accelerate.accelerator.log:61] [PID:937022] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.
{'eval_loss': 0.35337600111961365, 'eval_runtime': 722.4221, 'eval_samples_per_second': 0.692, 'eval_steps_per_second': 0.173, 'epoch': 0.25}

 27%|█████████████████████████████████████████████████████▊                                                                                                                                              | 14/51 [43:35<3:08:12, 305.20s/it]


 31%|█████████████████████████████████████████████████████████████▍                                                                                                                                      | 16/51 [46:25<1:52:14, 192.41s/it]

 33%|█████████████████████████████████████████████████████████████████▎                                                                                                                                  | 17/51 [47:49<1:30:38, 159.96s/it]

 35%|█████████████████████████████████████████████████████████████████████▏                                                                                                                              | 18/51 [49:14<1:15:28, 137.23s/it]

 37%|█████████████████████████████████████████████████████████████████████████                                                                                                                           | 19/51 [50:37<1:04:38, 121.19s/it]

 39%|█████████████████████████████████████████████████████████████████████████████▋                                                                                                                        | 20/51 [52:02<56:52, 110.07s/it]

 41%|█████████████████████████████████████████████████████████████████████████████████▌                                                                                                                    | 21/51 [53:26<51:07, 102.27s/it]

 43%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                 | 22/51 [54:50<46:48, 96.84s/it]
{'loss': 0.2655, 'grad_norm': 0.136988565325737, 'learning_rate': 0.0001606225410966638, 'epoch': 0.43}

 45%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                             | 23/51 [56:15<43:31, 93.28s/it]

 47%|█████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                         | 24/51 [57:39<40:45, 90.57s/it]

 49%|█████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                     | 25/51 [59:03<38:21, 88.52s/it]

 51%|████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                | 26/51 [1:00:27<36:19, 87.18s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 124/125 [11:48<00:05,  5.71s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [11:54<00:00,  5.73s/it]

{'eval_loss': 0.2804560363292694, 'eval_runtime': 719.893, 'eval_samples_per_second': 0.695, 'eval_steps_per_second': 0.174, 'epoch': 0.51}

 53%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                           | 27/51 [1:13:50<2:00:47, 301.97s/it]

 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                       | 28/51 [1:15:14<1:30:41, 236.58s/it]

 57%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                   | 29/51 [1:16:38<1:10:00, 190.92s/it]

 59%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                | 30/51 [1:18:02<55:36, 158.87s/it]


 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                         | 32/51 [1:20:52<38:18, 120.97s/it]
{'loss': 0.2864, 'grad_norm': 0.1066073328256607, 'learning_rate': 8.853165746015997e-05, 'epoch': 0.63}

 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                     | 33/51 [1:22:15<32:55, 109.75s/it]

 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                 | 34/51 [1:23:39<28:53, 101.99s/it]

 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                             | 35/51 [1:25:03<25:45, 96.62s/it]

 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                          | 36/51 [1:26:27<23:12, 92.81s/it]

 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                      | 37/51 [1:27:51<21:02, 90.16s/it]

 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                  | 38/51 [1:29:15<19:08, 88.32s/it]

 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                              | 39/51 [1:30:39<17:24, 87.06s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 124/125 [11:49<00:05,  5.79s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [11:55<00:00,  5.80s/it]

{'eval_loss': 0.2549363672733307, 'eval_runtime': 721.6037, 'eval_samples_per_second': 0.693, 'eval_steps_per_second': 0.173, 'epoch': 0.76}


 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                      | 41/51 [1:45:29<39:31, 237.10s/it]
{'loss': 0.2924, 'grad_norm': 0.1298089474439621, 'learning_rate': 2.794784063992131e-05, 'epoch': 0.8}

 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                  | 42/51 [1:46:53<28:39, 191.07s/it]


 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                              | 43/51 [1:48:16<21:11, 158.89s/it]
{'loss': 0.2665, 'grad_norm': 0.19766321778297424, 'learning_rate': 1.4043039301279903e-05, 'epoch': 0.86}
 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                           | 44/51 [1:49:40<15:53, 136.16s/it]


 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 46/51 [1:52:26<09:05, 109.19s/it]

 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 47/51 [1:53:50<06:46, 101.67s/it]

 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 48/51 [1:55:14<04:48, 96.20s/it]
{'loss': 0.3029, 'grad_norm': 0.10212480276823044, 'learning_rate': 2.6304576122221035e-06, 'epoch': 0.94}

 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 49/51 [1:56:37<03:04, 92.47s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [1:59:26<00:00, 88.29s/it]/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 0.2899, 'grad_norm': 0.12377654016017914, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 7171.1423, 'train_samples_per_second': 0.628, 'train_steps_per_second': 0.007, 'train_loss': 0.3368540175405203, 'epoch': 1.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [1:59:29<00:00, 140.58s/it]
/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2024-07-02 15:24:36,194] [INFO] [axolotl.train.log:61] [PID:937022] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/dscoder-code-ir-4













adapter_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 848M/848M [00:28<00:00, 29.6MB/s]
(PeftModelForCausalLM(   (base_model): LoraModel(     (model): LlamaForCausalLM(       (model): LlamaModel(         (embed_tokens): ModulesToSaveWrapper(           (original_module): Embedding(32256, 4096)           (modules_to_save): ModuleDict(             (default): Embedding(32256, 4096)           )         )         (layers): ModuleList(           (0-31): 32 x LlamaDecoderLayer(             (self_attn): LlamaAttention(               (q_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (k_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (v_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (o_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (rotary_emb): LlamaLinearScalingRotaryEmbedding()             )             (mlp): LlamaMLP(               (gate_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (up_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (down_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=11008, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (act_fn): SiLU()             )             (input_layernorm): LlamaRMSNorm()             (post_attention_layernorm): LlamaRMSNorm()           )         )         (norm): LlamaRMSNorm()       )       (lm_head): ModulesToSaveWrapper(         (original_module): Linear(in_features=4096, out_features=32256, bias=False)         (modules_to_save): ModuleDict(           (default): Linear(in_features=4096, out_features=32256, bias=False)         )       )     )   ) ), LlamaTokenizerFast(name_or_path='deepseek-ai/deepseek-coder-6.7b-instruct', vocab_size=32000, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|begin_of_sentence|>', 'eos_token': '<|end_of_sentence|>', 'pad_token': '<|end_of_sentence|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ 	32000: AddedToken("õ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32001: AddedToken("÷", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32002: AddedToken("Á", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32003: AddedToken("ý", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32004: AddedToken("À", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32005: AddedToken("ÿ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32006: AddedToken("ø", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32007: AddedToken("ú", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32008: AddedToken("þ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32009: AddedToken("ü", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32010: AddedToken("ù", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32011: AddedToken("ö", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32012: AddedToken("û", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32013: AddedToken("<｜begin▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32014: AddedToken("<｜end▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32015: AddedToken("<｜fim▁hole｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32016: AddedToken("<｜fim▁begin｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32017: AddedToken("<｜fim▁end｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32018: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32019: AddedToken("<|User|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32020: AddedToken("<|Assistant|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32021: AddedToken("<|EOT|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32022: AddedToken("<|begin_of_sentence|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 	32023: AddedToken("<|end_of_sentence|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), })