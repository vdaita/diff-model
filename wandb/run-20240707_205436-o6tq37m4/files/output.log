[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                    | 0/46 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-07-07 20:54:36,957] [INFO] [axolotl.callbacks.on_train_begin:770] [PID:2531046] [RANK:0] The Axolotl config has been saved to the WandB run under files.
[2024-07-07 20:54:36,960] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:2531046] [RANK:0] packing_efficiency_estimate: 0.96 total_num_tokens per device: 1471737
[2024-07-07 20:54:36,962] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:2531046] [RANK:0] packing_efficiency_estimate: 0.96 total_num_tokens per device: 1471737
{'loss': 0.3814, 'grad_norm': 0.3067726492881775, 'learning_rate': 2e-05, 'epoch': 0.02}
  2%|▉                                         | 1/46 [01:21<1:00:55, 81.24s/it]



























































































































 99%|████████████████████████████████████████▋| 124/125 [11:51<00:05,  5.76s/it]

100%|█████████████████████████████████████████| 125/125 [11:57<00:00,  5.76s/it]

{'eval_loss': 0.41920241713523865, 'eval_runtime': 723.4893, 'eval_samples_per_second': 0.691, 'eval_steps_per_second': 0.173, 'epoch': 0.02}
[2024-07-07 21:09:25,819] [INFO] [axolotl.callbacks.on_step_end:125] [PID:2531046] [RANK:0] GPU memory usage while training: 9.350GB (+12.697GB cache)

  4%|█▊                                       | 2/46 [14:48<6:12:55, 508.52s/it]

  7%|██▋                                      | 3/46 [16:14<3:46:00, 315.36s/it]

  9%|███▌                                     | 4/46 [17:40<2:37:18, 224.74s/it]

 11%|████▍                                    | 5/46 [19:05<1:59:16, 174.55s/it]

 13%|█████▎                                   | 6/46 [20:31<1:36:14, 144.36s/it]

 15%|██████▏                                  | 7/46 [21:56<1:21:18, 125.09s/it]

 17%|███████▏                                 | 8/46 [23:22<1:11:19, 112.61s/it]

 20%|████████                                 | 9/46 [24:48<1:04:14, 104.17s/it]

 22%|█████████▎                                 | 10/46 [26:14<59:04, 98.45s/it]

 24%|██████████▎                                | 11/46 [27:39<55:08, 94.54s/it]

 26%|███████████▏                               | 12/46 [29:06<52:10, 92.08s/it]



























































































































 99%|████████████████████████████████████████▋| 124/125 [12:02<00:05,  5.83s/it]

100%|█████████████████████████████████████████| 125/125 [12:08<00:00,  5.84s/it]


 28%|███████████▎                            | 13/46 [42:45<2:51:43, 312.23s/it]
{'loss': 0.2206, 'grad_norm': 0.8077993392944336, 'learning_rate': 0.00019659258262890683, 'epoch': 0.28}


 33%|█████████████                           | 15/46 [45:35<1:41:14, 195.94s/it]
{'loss': 0.2016, 'grad_norm': 0.6327354311943054, 'learning_rate': 0.000190630778703665, 'epoch': 0.33}


 37%|██████████████▊                         | 17/46 [48:25<1:07:16, 139.20s/it]
{'loss': 0.1866, 'grad_norm': 0.1226968914270401, 'learning_rate': 0.0001819152044288992, 'epoch': 0.37}

 39%|████████████████▍                         | 18/46 [49:51<57:29, 123.20s/it]

 41%|█████████████████▎                        | 19/46 [51:17<50:23, 111.97s/it]


 46%|███████████████████▋                       | 21/46 [54:08<41:02, 98.51s/it]

 48%|████████████████████▌                      | 22/46 [55:34<37:53, 94.72s/it]
{'loss': 0.1838, 'grad_norm': 0.06119630113244057, 'learning_rate': 0.00015000000000000001, 'epoch': 0.48}

 50%|█████████████████████▌                     | 23/46 [56:59<35:14, 91.95s/it]

 52%|██████████████████████▍                    | 24/46 [58:25<33:01, 90.08s/it]




























































































































100%|█████████████████████████████████████████| 125/125 [12:06<00:00,  5.84s/it]

[2024-07-07 22:05:09,968] [INFO] [accelerate.accelerator.log:61] [PID:2531046] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.
{'eval_loss': 0.16415323317050934, 'eval_runtime': 731.9772, 'eval_samples_per_second': 0.683, 'eval_steps_per_second': 0.171, 'epoch': 0.52}


 57%|█████████████████████▍                | 26/46 [1:13:27<1:20:25, 241.25s/it]
{'loss': 0.1487, 'grad_norm': 0.06074187904596329, 'learning_rate': 0.00011736481776669306, 'epoch': 0.57}

 59%|██████████████████████▎               | 27/46 [1:14:52<1:01:35, 194.50s/it]

 61%|████████████████████████▎               | 28/46 [1:16:18<48:33, 161.88s/it]

 63%|█████████████████████████▏              | 29/46 [1:17:44<39:23, 139.06s/it]

 65%|██████████████████████████              | 30/46 [1:19:10<32:49, 123.11s/it]

 67%|██████████████████████████▉             | 31/46 [1:20:35<27:56, 111.74s/it]

 70%|███████████████████████████▊            | 32/46 [1:22:01<24:14, 103.93s/it]

 72%|█████████████████████████████▍           | 33/46 [1:23:26<21:20, 98.47s/it]

 74%|██████████████████████████████▎          | 34/46 [1:24:52<18:56, 94.69s/it]

 76%|███████████████████████████████▏         | 35/46 [1:26:18<16:53, 92.15s/it]

 78%|████████████████████████████████         | 36/46 [1:27:45<15:04, 90.48s/it]



























































































































 99%|████████████████████████████████████████▋| 124/125 [12:05<00:05,  5.85s/it]

100%|█████████████████████████████████████████| 125/125 [12:11<00:00,  5.85s/it]

{'eval_loss': 0.15179982781410217, 'eval_runtime': 737.0011, 'eval_samples_per_second': 0.678, 'eval_steps_per_second': 0.17, 'epoch': 0.78}

 80%|████████████████████████████████▏       | 37/46 [1:41:28<46:31, 310.17s/it]

 83%|█████████████████████████████████       | 38/46 [1:42:54<32:23, 242.94s/it]

 85%|█████████████████████████████████▉      | 39/46 [1:44:20<22:50, 195.81s/it]

 87%|██████████████████████████████████▊     | 40/46 [1:45:46<16:16, 162.82s/it]

 89%|███████████████████████████████████▋    | 41/46 [1:47:11<11:38, 139.65s/it]

 91%|████████████████████████████████████▌   | 42/46 [1:48:38<08:14, 123.74s/it]

 93%|█████████████████████████████████████▍  | 43/46 [1:50:04<05:37, 112.55s/it]

 96%|██████████████████████████████████████▎ | 44/46 [1:51:31<03:29, 104.71s/it]


100%|█████████████████████████████████████████| 46/46 [1:54:23<00:00, 95.31s/it]
100%|█████████████████████████████████████████| 46/46 [1:54:23<00:00, 95.31s/it]/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'train_runtime': 6868.5331, 'train_samples_per_second': 0.655, 'train_steps_per_second': 0.007, 'train_loss': 0.22052575064742047, 'epoch': 1.0}
100%|████████████████████████████████████████| 46/46 [1:54:27<00:00, 149.29s/it]
[2024-07-07 22:49:24,098] [INFO] [axolotl.train.log:61] [PID:2531046] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/dscoder-code-chunked
/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(









adapter_model.bin: 100%|█████████████████████| 848M/848M [00:19<00:00, 43.6MB/s]
(PeftModelForCausalLM(   (base_model): LoraModel(     (model): LlamaForCausalLM(       (model): LlamaModel(         (embed_tokens): ModulesToSaveWrapper(           (original_module): Embedding(32256, 4096)           (modules_to_save): ModuleDict(             (default): Embedding(32256, 4096)           )         )         (layers): ModuleList(           (0-31): 32 x LlamaDecoderLayer(             (self_attn): LlamaAttention(               (q_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (k_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (v_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (o_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (rotary_emb): LlamaLinearScalingRotaryEmbedding()             )             (mlp): LlamaMLP(               (gate_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (up_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (down_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=11008, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (act_fn): SiLU()             )             (input_layernorm): LlamaRMSNorm()             (post_attention_layernorm): LlamaRMSNorm()           )         )         (norm): LlamaRMSNorm()       )       (lm_head): ModulesToSaveWrapper(         (original_module): Linear(in_features=4096, out_features=32256, bias=False)         (modules_to_save): ModuleDict(           (default): Linear(in_features=4096, out_features=32256, bias=False)         )       )     )   ) ), LlamaTokenizerFast(name_or_path='deepseek-ai/deepseek-coder-6.7b-instruct', vocab_size=32000, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|begin_of_sentence|>', 'eos_token': '<|end_of_sentence|>', 'pad_token': '<|end_of_sentence|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ 	32000: AddedToken("õ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32001: AddedToken("÷", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32002: AddedToken("Á", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32003: AddedToken("ý", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32004: AddedToken("À", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32005: AddedToken("ÿ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32006: AddedToken("ø", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32007: AddedToken("ú", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32008: AddedToken("þ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32009: AddedToken("ü", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32010: AddedToken("ù", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32011: AddedToken("ö", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32012: AddedToken("û", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32013: AddedToken("<｜begin▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32014: AddedToken("<｜end▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32015: AddedToken("<｜fim▁hole｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32016: AddedToken("<｜fim▁begin｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32017: AddedToken("<｜fim▁end｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32018: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32019: AddedToken("<|User|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32020: AddedToken("<|Assistant|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32021: AddedToken("<|EOT|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32022: AddedToken("<|begin_of_sentence|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 	32023: AddedToken("<|end_of_sentence|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), })