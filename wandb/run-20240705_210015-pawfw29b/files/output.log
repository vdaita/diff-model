[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                                                           | 0/43 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-07-05 21:00:16,069] [INFO] [axolotl.callbacks.on_train_begin:770] [PID:1638947] [RANK:0] The Axolotl config has been saved to the WandB run under files.
[2024-07-05 21:00:16,071] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1638947] [RANK:0] packing_efficiency_estimate: 0.96 total_num_tokens per device: 1388412
[2024-07-05 21:00:16,073] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:1638947] [RANK:0] packing_efficiency_estimate: 0.96 total_num_tokens per device: 1388412
  2%|█▉                                                                                 | 1/43 [01:20<56:26, 80.64s/it]
{'loss': 0.3241, 'grad_norm': 0.41485148668289185, 'learning_rate': 2e-05, 'epoch': 0.02}




























































































































100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [11:38<00:00,  5.62s/it]

[2024-07-05 21:13:17,282] [INFO] [accelerate.accelerator.log:61] [PID:1638947] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.

  5%|███▋                                                                            | 2/43 [14:27<5:38:55, 496.00s/it]
[2024-07-05 21:14:43,457] [INFO] [axolotl.callbacks.on_step_end:125] [PID:1638947] [RANK:0] GPU memory usage while training: 9.345GB (+12.748GB cache)
{'loss': 0.3362, 'grad_norm': 0.48898622393608093, 'learning_rate': 4e-05, 'epoch': 0.05}

  7%|█████▌                                                                          | 3/43 [15:50<3:24:56, 307.41s/it]

  9%|███████▍                                                                        | 4/43 [17:13<2:22:22, 219.04s/it]

 12%|█████████▎                                                                      | 5/43 [18:37<1:47:44, 170.12s/it]

 14%|███████████▏                                                                    | 6/43 [20:00<1:26:39, 140.54s/it]

 16%|█████████████                                                                   | 7/43 [21:23<1:13:02, 121.73s/it]

 19%|██████████████▉                                                                 | 8/43 [22:46<1:03:52, 109.51s/it]

 21%|█████████████████▏                                                                | 9/43 [24:10<57:24, 101.32s/it]

 23%|███████████████████                                                               | 10/43 [25:33<52:38, 95.71s/it]

 26%|████████████████████▉                                                             | 11/43 [26:56<48:58, 91.83s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████▎| 124/125 [11:40<00:05,  5.70s/it]

100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [11:46<00:00,  5.71s/it]

{'eval_loss': 0.23027436435222626, 'eval_runtime': 711.8751, 'eval_samples_per_second': 0.702, 'eval_steps_per_second': 0.176, 'epoch': 0.25}

 28%|██████████████████████                                                         | 12/43 [40:12<2:38:07, 306.03s/it]

 30%|███████████████████████▉                                                       | 13/43 [41:35<1:59:18, 238.62s/it]

 33%|█████████████████████████▋                                                     | 14/43 [42:59<1:32:41, 191.77s/it]

 35%|███████████████████████████▌                                                   | 15/43 [44:22<1:14:17, 159.19s/it]

 37%|█████████████████████████████▍                                                 | 16/43 [45:46<1:01:22, 136.39s/it]

 40%|████████████████████████████████                                                 | 17/43 [47:10<52:15, 120.59s/it]


 44%|███████████████████████████████████▊                                             | 19/43 [49:58<40:45, 101.89s/it]
{'loss': 0.1896, 'grad_norm': 0.07485965639352798, 'learning_rate': 0.00016548607339452853, 'epoch': 0.44}

 47%|██████████████████████████████████████▏                                           | 20/43 [51:22<37:03, 96.66s/it]

 49%|████████████████████████████████████████                                          | 21/43 [52:47<34:06, 93.02s/it]

 51%|█████████████████████████████████████████▉                                        | 22/43 [54:11<31:40, 90.52s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████▎| 124/125 [11:45<00:05,  5.71s/it]

100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [11:51<00:00,  5.72s/it]

{'eval_loss': 0.17712850868701935, 'eval_runtime': 716.8789, 'eval_samples_per_second': 0.697, 'eval_steps_per_second': 0.174, 'epoch': 0.51}

 53%|█████████████████████████████████████████▏                                   | 23/43 [1:07:32<1:41:14, 303.74s/it]

 56%|██████████████████████████████████████████▉                                  | 24/43 [1:08:57<1:15:20, 237.91s/it]

 58%|█████████████████████████████████████████████▉                                 | 25/43 [1:10:20<57:29, 191.62s/it]

 60%|███████████████████████████████████████████████▊                               | 26/43 [1:11:44<45:07, 159.27s/it]

 63%|█████████████████████████████████████████████████▌                             | 27/43 [1:13:08<36:26, 136.63s/it]

 65%|███████████████████████████████████████████████████▍                           | 28/43 [1:14:31<30:10, 120.68s/it]


 70%|███████████████████████████████████████████████████████                        | 30/43 [1:17:19<22:05, 101.97s/it]
{'loss': 0.1727, 'grad_norm': 0.11059073358774185, 'learning_rate': 6.729320366825784e-05, 'epoch': 0.69}

 72%|█████████████████████████████████████████████████████████▋                      | 31/43 [1:18:43<19:18, 96.58s/it]

 74%|███████████████████████████████████████████████████████████▌                    | 32/43 [1:20:07<16:58, 92.64s/it]

 77%|█████████████████████████████████████████████████████████████▍                  | 33/43 [1:21:31<15:00, 90.02s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████▎| 124/125 [11:45<00:05,  5.70s/it]

100%|████████████████████████████████████████████████████████████████████████████████| 125/125 [11:51<00:00,  5.71s/it]

{'eval_loss': 0.16340960562229156, 'eval_runtime': 716.9647, 'eval_samples_per_second': 0.697, 'eval_steps_per_second': 0.174, 'epoch': 0.76}


 81%|████████████████████████████████████████████████████████████████▎              | 35/43 [1:36:17<31:42, 237.79s/it]

 84%|██████████████████████████████████████████████████████████████████▏            | 36/43 [1:37:41<22:21, 191.68s/it]
{'loss': 0.1594, 'grad_norm': 0.07909317314624786, 'learning_rate': 2.139469052572127e-05, 'epoch': 0.83}

 86%|███████████████████████████████████████████████████████████████████▉           | 37/43 [1:39:05<15:57, 159.51s/it]

 88%|█████████████████████████████████████████████████████████████████████▊         | 38/43 [1:40:30<11:25, 137.08s/it]

 91%|███████████████████████████████████████████████████████████████████████▋       | 39/43 [1:41:54<08:05, 121.34s/it]

 93%|█████████████████████████████████████████████████████████████████████████▍     | 40/43 [1:43:19<05:30, 110.28s/it]

 95%|███████████████████████████████████████████████████████████████████████████▎   | 41/43 [1:44:43<03:25, 102.58s/it]

 98%|██████████████████████████████████████████████████████████████████████████████▏ | 42/43 [1:46:08<01:37, 97.14s/it]
100%|████████████████████████████████████████████████████████████████████████████████| 43/43 [1:47:32<00:00, 93.26s/it]/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'train_runtime': 6457.4759, 'train_samples_per_second': 0.697, 'train_steps_per_second': 0.007, 'train_loss': 0.21977483013341592, 'epoch': 0.99}
100%|███████████████████████████████████████████████████████████████████████████████| 43/43 [1:47:36<00:00, 150.14s/it]
[2024-07-05 22:48:14,327] [INFO] [axolotl.train.log:61] [PID:1638947] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/dscoder-code-ellipsis
/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(










adapter_model.bin: 100%|████████████████████████████████████████████████████████████| 848M/848M [00:19<00:00, 42.4MB/s]