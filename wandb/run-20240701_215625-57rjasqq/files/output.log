[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
  0%|                                                                                                                                                                                                                | 0/39 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/vijay/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/vijay/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-07-01 21:56:25,519] [INFO] [axolotl.callbacks.on_train_begin:770] [PID:4133120] [RANK:0] The Axolotl config has been saved to the WandB run under files.
[2024-07-01 21:56:25,522] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:4133120] [RANK:0] packing_efficiency_estimate: 0.97 total_num_tokens per device: 1274536
[2024-07-01 21:56:25,523] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:4133120] [RANK:0] packing_efficiency_estimate: 0.97 total_num_tokens per device: 1274536
  3%|█████▏                                                                                                                                                                                                  | 1/39 [01:20<51:16, 80.95s/it]
{'loss': 0.6921, 'grad_norm': 0.5078456401824951, 'learning_rate': 2e-05, 'epoch': 0.03}



























































































































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 124/125 [11:55<00:05,  5.77s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [12:01<00:00,  5.79s/it]

{'eval_loss': 0.7832242250442505, 'eval_runtime': 726.984, 'eval_samples_per_second': 0.688, 'eval_steps_per_second': 0.172, 'epoch': 0.03}
[2024-07-01 22:11:18,010] [INFO] [axolotl.callbacks.on_step_end:125] [PID:4133120] [RANK:0] GPU memory usage while training: 9.361GB (+12.125GB cache)

  5%|██████████                                                                                                                                                                                           | 2/39 [14:52<5:14:56, 510.71s/it]

  8%|███████████████▏                                                                                                                                                                                     | 3/39 [16:18<3:10:10, 316.96s/it]

 10%|████████████████████▏                                                                                                                                                                                | 4/39 [17:45<2:11:45, 225.86s/it]

 13%|█████████████████████████▎                                                                                                                                                                           | 5/39 [19:11<1:39:23, 175.40s/it]

 15%|██████████████████████████████▎                                                                                                                                                                      | 6/39 [20:36<1:19:39, 144.82s/it]

 18%|███████████████████████████████████▎                                                                                                                                                                 | 7/39 [22:01<1:06:48, 125.26s/it]

 21%|████████████████████████████████████████▊                                                                                                                                                              | 8/39 [23:26<58:07, 112.51s/it]

 23%|█████████████████████████████████████████████▉                                                                                                                                                         | 9/39 [24:51<51:56, 103.88s/it]
 26%|███████████████████████████████████████████████████                                                                                                                                                    | 10/39 [26:16<47:24, 98.10s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 124/125 [11:59<00:05,  5.82s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [12:05<00:00,  5.83s/it]

{'eval_loss': 0.5221271514892578, 'eval_runtime': 730.9918, 'eval_samples_per_second': 0.684, 'eval_steps_per_second': 0.171, 'epoch': 0.25}


 31%|████████████████████████████████████████████████████████████▎                                                                                                                                       | 12/39 [41:19<1:51:20, 247.44s/it]
{'loss': 0.4582, 'grad_norm': 0.7742004990577698, 'learning_rate': 0.00019766205557100868, 'epoch': 0.31}

 33%|█████████████████████████████████████████████████████████████████▎                                                                                                                                  | 13/39 [42:44<1:25:56, 198.33s/it]

 36%|██████████████████████████████████████████████████████████████████████▎                                                                                                                             | 14/39 [44:10<1:08:30, 164.40s/it]


 41%|█████████████████████████████████████████████████████████████████████████████████▏                                                                                                                    | 16/39 [47:01<47:33, 124.06s/it]
{'loss': 0.3383, 'grad_norm': 5.251585960388184, 'learning_rate': 0.00017960930657056438, 'epoch': 0.41}

 44%|██████████████████████████████████████████████████████████████████████████████████████▎                                                                                                               | 17/39 [48:27<41:17, 112.60s/it]

 46%|███████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                          | 18/39 [49:53<36:35, 104.54s/it]

 49%|████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 19/39 [51:19<33:00, 99.03s/it]

 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                 | 20/39 [52:45<30:03, 94.91s/it]



























































































































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 124/125 [11:57<00:05,  5.80s/it]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [12:03<00:00,  5.82s/it]


 54%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                         | 21/39 [1:06:19<1:33:14, 310.82s/it]

 56%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                    | 22/39 [1:07:45<1:08:57, 243.36s/it]
{'loss': 0.2618, 'grad_norm': 0.10277070850133896, 'learning_rate': 0.00012675283385292212, 'epoch': 0.56}

 59%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                | 23/39 [1:09:10<52:16, 196.01s/it]

 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                           | 24/39 [1:10:36<40:41, 162.80s/it]


 67%|██████████████████████████████████████████████████████████████████████████████████████████▋                                             | 26/39 [1:13:27<26:44, 123.45s/it]
{'loss': 0.2786, 'grad_norm': 0.19100333750247955, 'learning_rate': 8.382180034472353e-05, 'epoch': 0.66}

 69%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 27/39 [1:14:53<22:24, 112.03s/it]

 72%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                      | 28/39 [1:16:18<19:04, 104.05s/it]

 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 29/39 [1:17:43<16:23, 98.38s/it]

 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                               | 30/39 [1:19:08<14:09, 94.41s/it]




























































































































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [12:03<00:00,  5.78s/it]
[2024-07-01 23:27:38,900] [INFO] [accelerate.accelerator.log:61] [PID:4133120] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.

{'eval_loss': 0.2676665186882019, 'eval_runtime': 728.8723, 'eval_samples_per_second': 0.686, 'eval_steps_per_second': 0.171, 'epoch': 0.76}


 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 32/39 [1:34:06<28:17, 242.51s/it]
{'loss': 0.2728, 'grad_norm': 0.16299675405025482, 'learning_rate': 2.7400450807686938e-05, 'epoch': 0.82}

 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 33/39 [1:35:32<19:31, 195.32s/it]

 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 34/39 [1:36:56<13:30, 162.11s/it]


 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 36/39 [1:39:45<06:07, 122.39s/it]

 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 37/39 [1:41:10<03:42, 111.14s/it]

 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 38/39 [1:42:34<01:43, 103.03s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [1:43:58<00:00, 97.29s/it]/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'loss': 0.3135, 'grad_norm': 0.07369281351566315, 'learning_rate': 0.0, 'epoch': 0.99}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [1:44:01<00:00, 160.04s/it]
{'train_runtime': 6242.7033, 'train_samples_per_second': 0.721, 'train_steps_per_second': 0.006, 'train_loss': 0.3998185858512536, 'epoch': 0.99}
[2024-07-01 23:40:52,109] [INFO] [axolotl.train.log:61] [PID:4133120] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/dscoder-code-ir-3
/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(









adapter_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 848M/848M [00:18<00:00, 45.4MB/s]
(PeftModelForCausalLM(   (base_model): LoraModel(     (model): LlamaForCausalLM(       (model): LlamaModel(         (embed_tokens): ModulesToSaveWrapper(           (original_module): Embedding(32256, 4096)           (modules_to_save): ModuleDict(             (default): Embedding(32256, 4096)           )         )         (layers): ModuleList(           (0-31): 32 x LlamaDecoderLayer(             (self_attn): LlamaAttention(               (q_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (k_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (v_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (o_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (rotary_emb): LlamaLinearScalingRotaryEmbedding()             )             (mlp): LlamaMLP(               (gate_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (up_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=4096, out_features=11008, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=4096, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=11008, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (down_proj): lora.Linear8bitLt(                 (base_layer): Linear8bitLt(in_features=11008, out_features=4096, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=11008, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=4096, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()               )               (act_fn): SiLU()             )             (input_layernorm): LlamaRMSNorm()             (post_attention_layernorm): LlamaRMSNorm()           )         )         (norm): LlamaRMSNorm()       )       (lm_head): ModulesToSaveWrapper(         (original_module): Linear(in_features=4096, out_features=32256, bias=False)         (modules_to_save): ModuleDict(           (default): Linear(in_features=4096, out_features=32256, bias=False)         )       )     )   ) ), LlamaTokenizerFast(name_or_path='deepseek-ai/deepseek-coder-6.7b-instruct', vocab_size=32000, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|begin_of_sentence|>', 'eos_token': '<|end_of_sentence|>', 'pad_token': '<|end_of_sentence|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ 	32000: AddedToken("õ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32001: AddedToken("÷", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32002: AddedToken("Á", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32003: AddedToken("ý", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32004: AddedToken("À", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32005: AddedToken("ÿ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32006: AddedToken("ø", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32007: AddedToken("ú", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32008: AddedToken("þ", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32009: AddedToken("ü", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32010: AddedToken("ù", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32011: AddedToken("ö", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32012: AddedToken("û", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32013: AddedToken("<｜begin▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32014: AddedToken("<｜end▁of▁sentence｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32015: AddedToken("<｜fim▁hole｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32016: AddedToken("<｜fim▁begin｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32017: AddedToken("<｜fim▁end｜>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32018: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32019: AddedToken("<|User|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32020: AddedToken("<|Assistant|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False), 	32021: AddedToken("<|EOT|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 	32022: AddedToken("<|begin_of_sentence|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 	32023: AddedToken("<|end_of_sentence|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), })